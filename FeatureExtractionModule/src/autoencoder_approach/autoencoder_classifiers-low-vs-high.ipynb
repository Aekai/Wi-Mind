{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "Exploring different classifiers with different autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoders:  \n",
    "[Undercomplete Autoencoder](#Undercomplete-Autoencoder)  \n",
    "[Sparse Autoencoder](#Sparse-Autoencoder)  \n",
    "[Deep Autoencoder](#Deep-Autoencoder)  \n",
    "[Contractive Autoencoder](#Contractive-Autoencoder)  \n",
    "\n",
    "classifiers:  \n",
    "[Simple dense layer](#Simple-dense-layer)  \n",
    "[LSTM-based classifier](#LSTM-based-classifier)  \n",
    "[kNN](#kNN)  \n",
    "[SVC](#SVC)  \n",
    "[Random Forest](#Random-Forest)  \n",
    "[XGBoost](#XGBoost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datareader # made by the previous author for reading the collected data\n",
    "import dataextractor # same as above\n",
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# need to disable eager execution for .get_weights() in contractive autoencoder loss to work\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "# required for the contractive autoencoder\n",
    "import tensorflow.keras.backend as K\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import warnings\n",
    "\n",
    "import talos\n",
    "from talos.utils import lr_normalizer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.keras.backend.set_floatx('float32') # call this, to set keras to use float32 to avoid a warning message\n",
    "metrics = ['accuracy']#,\n",
    "#            keras.metrics.TruePositives(),\n",
    "#            keras.metrics.FalsePositives(),\n",
    "#            keras.metrics.TrueNegatives(),\n",
    "#            keras.metrics.FalseNegatives()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/ageron/handson-ml/blob/master/extra_tensorflow_reproducibility.ipynb\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                        inter_op_parallelism_threads=1)\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    #... this will run single threaded\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(4)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the notebook in the terminal with \"PYTHONHASHSEED=0 jupyter notebook\" \n",
    "# or in anaconda \"set PYTHONHASHSEED=0\" then start jupyter notebook\n",
    "import os\n",
    "if os.environ.get(\"PYTHONHASHSEED\") != \"0\":\n",
    "    raise Exception(\"You must set PYTHONHASHSEED=0 when starting the Jupyter server to get reproducible results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original author's code, just copied into separate cells of this jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_busy_vs_relax_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data from either 'on task' or 'relax' time frames and their class (0 or 1).\n",
    "    TODO: join functions\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### task versus relax (1 sample each)\n",
    "        dataextract = dataextractor.DataExtractor(data[0][busy_n[task_num_table][0]:busy_n[task_num_table][1]],\n",
    "                                                  data[1][busy_n[task_num_table][0]:busy_n[task_num_table][1]],\n",
    "                                                  samp_rate)\n",
    "\n",
    "        dataextract_relax = dataextractor.DataExtractor(data[0][relax_n[task_num_table][0]:relax_n[task_num_table][1]],\n",
    "                                                        data[1][relax_n[task_num_table][0]:relax_n[task_num_table][1]],\n",
    "                                                        samp_rate)\n",
    "        try:\n",
    "            tasks_data = np.vstack((tasks_data, dataextract.y[-samp_rate * seconds:]))\n",
    "            tasks_y = np.vstack((tasks_y, 1))\n",
    "            tasks_data = np.vstack((tasks_data, dataextract_relax.y[-samp_rate * seconds:]))\n",
    "            tasks_y = np.vstack((tasks_y, 0))\n",
    "        except ValueError:\n",
    "            continue\n",
    "#             print(ident)  # ignore short windows\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engagement_increase_vs_decrease_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data from either engagement 'increase' or 'decrease' time frames and their class (0 or 1).\n",
    "    TODO: join functions\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds * samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### engagement increase / decrease\n",
    "        if task_num_table == 0:\n",
    "            continue\n",
    "        mid = int((relax_n[task_num_table][0] + relax_n[task_num_table][1])/2)\n",
    "        length = int(samp_rate*30)\n",
    "        for j in range(10):\n",
    "            new_end = int(mid-j*samp_rate)\n",
    "\n",
    "            new_start2 = int(mid+j*samp_rate)\n",
    "\n",
    "            dataextract_decrease = dataextractor.DataExtractor(data[0][new_end - length:new_end],\n",
    "                                                               data[1][new_end-length:new_end],\n",
    "                                                               samp_rate)\n",
    "\n",
    "            dataextract_increase = dataextractor.DataExtractor(data[0][new_start2: new_start2 + length],\n",
    "                                                               data[1][new_start2: new_start2 + length], samp_rate)\n",
    "\n",
    "            try:\n",
    "                tasks_data = np.vstack((tasks_data, dataextract_increase.y))\n",
    "                tasks_y = np.vstack((tasks_y, 1))\n",
    "                tasks_data = np.vstack((tasks_data, dataextract_decrease.y))\n",
    "                tasks_y = np.vstack((tasks_y, 0))\n",
    "            except ValueError:\n",
    "                print(ident)  # ignore short windows\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_complexities_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data along with task complexity class.\n",
    "    TODO: join functions. Add parameter to choose different task types and complexities\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### task complexity classification\n",
    "        if cog_res['task_complexity'][task_num_table] == 'medium':\n",
    "            continue\n",
    "        # if cog_res['task_label'][task_num_table] == 'FA' or cog_res['task_label'][task_num_table] == 'HP':\n",
    "        #     continue\n",
    "        if cog_res['task_label'][task_num_table] != 'NC':\n",
    "            continue\n",
    "        map_compl = {\n",
    "            'low': 0,\n",
    "            'medium': 2,\n",
    "            'high': 1\n",
    "        }\n",
    "        for j in range(10):\n",
    "            new_end = int(busy_n[task_num_table][1] - j * samp_rate)\n",
    "            new_start = int(new_end - samp_rate*30)\n",
    "            dataextract = dataextractor.DataExtractor(data[0][new_start:new_end],\n",
    "                                                      data[1][new_start:new_end], samp_rate)\n",
    "            try:\n",
    "                tasks_data = np.vstack((tasks_data, dataextract.y))\n",
    "                tasks_y = np.vstack((tasks_y, map_compl.get(cog_res['task_complexity'][task_num_table])))\n",
    "            except ValueError:\n",
    "                print(ident)\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TLX_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data along with task load index class.\n",
    "    TODO: join functions. Add parameter to choose different task types and complexities\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### task load index\n",
    "        if cog_res['task_complexity'][task_num_table] == 'medium' or cog_res['task_label'][task_num_table] != 'PT':\n",
    "            continue\n",
    "        for j in range(10):\n",
    "            new_end = int(busy_n[task_num_table][1] - j * samp_rate)\n",
    "            new_start = int(new_end - samp_rate*30)\n",
    "            dataextract = dataextractor.DataExtractor(data[0][new_start:new_end],\n",
    "                                                      data[1][new_start:new_end], samp_rate)\n",
    "            try:\n",
    "                tasks_data = np.vstack((tasks_data, dataextract.y))\n",
    "                tasks_y = np.vstack((tasks_y, cog_res['task_load_index'][task_num_table]))\n",
    "            except ValueError:\n",
    "                print(ident)\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_idents(path, idents, seconds):\n",
    "    \"\"\"Go through all user data and take out windows of only <seconds> long time frames,\n",
    "    along with the given class (from 'divide_each_task' function).\n",
    "    \"\"\"\n",
    "    samp_rate = 43  # hard-coded sample rate\n",
    "    data, ys = np.empty((0, samp_rate*seconds)), np.empty((0, 1))\n",
    "    for i in idents:\n",
    "        x, y = get_busy_vs_relax_timeframes(path, i, seconds) # either 'get_busy_vs_relax_timeframes',\n",
    "        # get_engagement_increase_vs_decrease_timeframes, get_task_complexities_timeframes or get_TLX_timeframes\n",
    "        # TODO: ^ modify, so that different functions can be accessible by parameter\n",
    "        data = np.vstack((data, x))\n",
    "        ys = np.vstack((ys, y))\n",
    "    return data, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, x_train, y_train, batch_size, epochs, x_valid, y_valid, x_test, y_test):\n",
    "    \"\"\"Train model with the given training, validation, and test set, with appropriate batch size and # epochs.\"\"\"\n",
    "    epoch_data = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_valid, y_valid), verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    acc = score[1]\n",
    "    score = score[0]\n",
    "    return score, acc, epoch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(x, maxlen):\n",
    "    \"\"\"Pad sequences (all have to be same length).\"\"\"\n",
    "    print('Pad sequences (samples x time)')\n",
    "    return sequence.pad_sequences(x, maxlen=maxlen, dtype=np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_complexities_timeframes_br_hb(path, ident, seconds):\n",
    "    \"\"\"Returns raw data along with task complexity class.\n",
    "    TODO: join functions. Add parameter to choose different task types and complexities\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "    breathing = np.empty((0,12))\n",
    "    heartbeat = np.empty((0,10))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    \n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "        tmp_tasks_data = np.empty((0, seconds*samp_rate))\n",
    "        tmp_tasks_y = np.empty((0, 1))\n",
    "        tmp_breathing = np.empty((0,12))\n",
    "        tmp_heartbeat = np.empty((0,10))\n",
    "        \n",
    "        ### task complexity classification\n",
    "        if cog_res['task_complexity'][task_num_table] == 'medium':\n",
    "            continue\n",
    "        # if cog_res['task_label'][task_num_table] == 'FA' or cog_res['task_label'][task_num_table] == 'HP':\n",
    "        #     continue\n",
    "#         if cog_res['task_label'][task_num_table] != 'NC':\n",
    "#             continue\n",
    "            \n",
    "        map_compl = {\n",
    "            'low': 0,\n",
    "            'medium': 2,\n",
    "            'high': 1\n",
    "        }\n",
    "        for j in range(10):\n",
    "            new_end = int(busy_n[task_num_table][1] - j * samp_rate)\n",
    "            new_start = int(new_end - samp_rate*30)\n",
    "            dataextract = dataextractor.DataExtractor(data[0][new_start:new_end],\n",
    "                                                      data[1][new_start:new_end], samp_rate)\n",
    "            # get extracted features for breathing\n",
    "            tmpBR = dataextract.extract_from_breathing_time(data[0][new_start:new_end],\n",
    "                                                                 data[1][new_start:new_end])\n",
    "            #get extracted features for heartbeat\n",
    "            tmpHB = dataextract.extract_from_heartbeat_time(data[0][new_start:new_end],\n",
    "                                                                 data[1][new_start:new_end])\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                tmp_tasks_data = np.vstack((tmp_tasks_data, dataextract.y[-samp_rate * seconds:]))\n",
    "                tmp_tasks_y = np.vstack((tmp_tasks_y, map_compl.get(cog_res['task_complexity'][task_num_table])))\n",
    "\n",
    "                tmp_breathing = np.vstack((tmp_breathing, tmpBR.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "                tmp_heartbeat = np.vstack((tmp_heartbeat, tmpHB.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "                \n",
    "            except ValueError:\n",
    "#                 print(ident)\n",
    "                continue\n",
    "\n",
    "            tasks_data = np.vstack((tasks_data, dataextract.y))\n",
    "            tasks_y = np.vstack((tasks_y, map_compl.get(cog_res['task_complexity'][task_num_table])))\n",
    "            breathing = np.vstack((breathing, tmpBR.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "            heartbeat = np.vstack((heartbeat, tmpHB.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "            \n",
    "    return tasks_data, tasks_y, breathing, heartbeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_idents_br_hb(path, idents, seconds):\n",
    "    \"\"\"Go through all user data and take out windows of only <seconds> long time frames,\n",
    "    along with the given class (from 'divide_each_task' function).\n",
    "    \"\"\"\n",
    "    samp_rate = 43  # hard-coded sample rate\n",
    "    data, ys = np.empty((0, samp_rate*seconds)), np.empty((0, 1))\n",
    "    brs = np.empty((0,12))\n",
    "    hbs = np.empty((0,10))\n",
    "    combined = np.empty((0,22))\n",
    "    \n",
    "    # was gettign some weird warnings; stack overflow said to ignore them\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        for i in idents:\n",
    "            #x, y, br, hb = get_busy_vs_relax_timeframes_br_hb(path, i, seconds) # either 'get_busy_vs_relax_timeframes',\n",
    "            # get_engagement_increase_vs_decrease_timeframes, get_task_complexities_timeframes or get_TLX_timeframes\n",
    "            x, y, br, hb = get_task_complexities_timeframes_br_hb(path, i, seconds)\n",
    "            \n",
    "            data = np.vstack((data, x))\n",
    "            ys = np.vstack((ys, y))\n",
    "            brs = np.vstack((brs, br))\n",
    "            hbs = np.vstack((hbs, hb))\n",
    "        combined = np.hstack((brs,hbs))\n",
    "    \n",
    "    return data, ys, brs, hbs, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(x_train, x_valid, x_test, standardScaler=True, minMaxScaler=True):\n",
    "    \n",
    "    # copy data variables\n",
    "    xt_train = x_train\n",
    "    xt_valid = x_valid\n",
    "    xt_test = x_test\n",
    "    \n",
    "    if standardScaler:\n",
    "        # Scale with standard scaler\n",
    "        sscaler = StandardScaler()\n",
    "        sscaler.fit(np.vstack((xt_train, xt_valid, xt_test)))\n",
    "        xt_train = sscaler.transform(xt_train)\n",
    "        xt_valid = sscaler.transform(xt_valid)\n",
    "        xt_test = sscaler.transform(xt_test)\n",
    "\n",
    "    if minMaxScaler:\n",
    "        # Scale with MinMax to range [0,1]\n",
    "        mmscaler = MinMaxScaler((0,1))\n",
    "        mmscaler.fit(np.vstack((xt_train, xt_valid, xt_test)))\n",
    "        xt_train = mmscaler.transform(xt_train)\n",
    "        xt_valid = mmscaler.transform(xt_valid)\n",
    "        xt_test = mmscaler.transform(xt_test)\n",
    "    \n",
    "    return xt_train, xt_valid, xt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accs is a dictionary which holds 1d arrays of accuracies in each key\n",
    "# except the key 'test id' which holds strings of the id which yielded the coresponding accuracies\n",
    "def print_accs_stats(accs):\n",
    "    # loop over each key\n",
    "    for key in accs:\n",
    "    \n",
    "        if (key == 'test id'):\n",
    "            # skip calculating ids\n",
    "            continue\n",
    "\n",
    "        # calculate and print some statistics\n",
    "        print(key, \"accuracies:\")\n",
    "        print(\"- min:\", np.min(accs[key]))\n",
    "        print(\"- max:\", np.max(accs[key]))\n",
    "        print(\"- mean:\", np.mean(accs[key]))\n",
    "        print(\"- median:\", np.median(accs[key]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undercomplete Autoencoder  \n",
    "from https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undercomplete_ae(x, encoding_dim=64, encoded_as_model=False):\n",
    "    # Simplest possible autoencoder from https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_data = Input(shape=x[0].shape, name=\"input\")\n",
    "    dropout = Dropout(0.25, name=\"dropout\")(input_data)\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation='relu', name=\"encoded\")(dropout)\n",
    "    \n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(x[0].shape[0], activation='sigmoid', name=\"decoded\")(encoded)\n",
    "\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "    \n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Autoencoder  \n",
    "from https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_ae(x, encoding_dim=64, encoded_as_model=False):\n",
    "    # Simplest possible autoencoder from https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_data = Input(shape=x[0].shape, name=\"input\")\n",
    "    dropout = Dropout(0.25, name=\"dropout\") (input_data)\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    # add a sparsity constraint\n",
    "    encoded = Dense(encoding_dim, activation='relu', name=\"encoded\",\n",
    "                    activity_regularizer=regularizers.l1(10e-5))(dropout)\n",
    "    \n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(x[0].shape[0], activation='sigmoid', name=\"decoded\")(encoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_data, decoded, name=\"sparse_ae\")\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "    \n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Autoencoder  \n",
    "from https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_ae(x, enc_layers=[512,256], encoding_dim=64, dec_layers=[256,512], encoded_as_model=False):\n",
    "    # From https://www.tensorflow.org/guide/keras/functional#use_the_same_graph_of_layers_to_define_multiple_models\n",
    "    input_data = keras.Input(shape=x[0].shape, name=\"normalized_signal\")\n",
    "    model = Dropout(0.25, name=\"dropout\", autocast=False)(input_data)\n",
    "    for i in enumerate(enc_layers):\n",
    "        model = Dense(i[1], activation=\"relu\", name=\"dense_enc_\" + str(i[0]+1))(model)\n",
    "    encoded_output = Dense(encoding_dim, activation=\"relu\", name=\"encoded_signal\")(model)\n",
    "\n",
    "    encoded = encoded_output\n",
    "\n",
    "    model = layers.Dense(dec_layers[0], activation=\"sigmoid\", name=\"dense_dec_1\")(encoded_output)\n",
    "    for i in enumerate(dec_layers[1:]):\n",
    "        model = Dense(i[1], activation=\"sigmoid\", name=\"dense_dec_\" + str(i[0]+2))(model)\n",
    "    decoded_output = Dense(x[0].shape[0], activation=\"sigmoid\", name=\"reconstructed_signal\")(model)\n",
    "    \n",
    "    autoencoder = Model(input_data, decoded_output, name=\"autoencoder\")\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "\n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contractive Autoencoder\n",
    "From: https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to be able to access the autoencoder in the loss funciton\n",
    "def loss_with_params(autoencoder):\n",
    "    # loss function from https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/\n",
    "    def contractive_loss(y_pred, y_true):\n",
    "\n",
    "        lam = 1e-4\n",
    "        mse = K.mean(K.square(y_true - y_pred), axis=1)\n",
    "\n",
    "        W = K.variable(value=autoencoder.get_layer('encoded').get_weights()[0])  # N x N_hidden\n",
    "        W = K.transpose(W)  # N_hidden x N\n",
    "        h = autoencoder.get_layer('encoded').output\n",
    "        dh = h * (1 - h)  # N_batch x N_hidden\n",
    "\n",
    "        # N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n",
    "        contractive = lam * K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n",
    "\n",
    "        return mse + contractive\n",
    "    return contractive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractive_ae(x, encoding_dim=64, encoded_as_model=False):\n",
    "    # From https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/\n",
    "\n",
    "    input_data = Input(shape=x[0].shape, name=\"input\")\n",
    "    encoded = Dense(encoding_dim, activation='sigmoid', name='encoded')(input_data)\n",
    "    outputs = Dense(x[0].shape[0], activation='linear', name=\"output\")(encoded)\n",
    "\n",
    "    autoencoder = Model(input_data, outputs, name=\"autoencoder\")\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss=loss_with_params(autoencoder), metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "    \n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary to store accuracies for comparison\n",
    "accuracies = {}\n",
    "\n",
    "# used for reading the data into an array\n",
    "seconds = 30  # time window length\n",
    "idents = ['2gu87', 'iz2ps', '1mpau', '7dwjy', '7swyk', '94mnx', 'bd47a', 'c24ur', 'ctsax', 'dkhty', 'e4gay',\n",
    "              'ef5rq', 'f1gjp', 'hpbxa', 'pmyfl', 'r89k1', 'tn4vl', 'td5pr', 'gyqu9', 'fzchw', 'l53hg', '3n2f9',\n",
    "              '62i9y']\n",
    "path = '../../../StudyData/'\n",
    "\n",
    "# change to len(idents) at the end to use all the data\n",
    "n = 3 #len(idents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_classifier(model, params):\n",
    "    \n",
    "    model = Dropout(params['dropout'], name='dropout_cl')(model)\n",
    "    model = Dense(params['hidden_size'], activation=params['activation'], name='dense_cl1')(model)\n",
    "    model = Dense(1, activation=params['last_activation'], name='dense_cl2')(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_classifier_base():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dropout': 0.24,\n",
    "    'optimizer': 'Adam',\n",
    "    'hidden_size': 64,\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'last_activation': 'sigmoid',\n",
    "    'activation': 'softmax',\n",
    "    'batch_size': 256,\n",
    "    'epochs': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 3 ; time elapsed: 0:00:00.007950\n",
      "WARNING:tensorflow:From D:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "iteration: 2 of 3 ; time elapsed: 0:03:20.478562\n",
      "iteration: 3 of 3 ; time elapsed: 0:06:47.710645\n",
      "Completed! Time elapsed: 0:10:25.853433\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['simple_dense'] = {}\n",
    "accs = accuracies['simple_dense']\n",
    "accs['phase'] = []\n",
    "accs['breathing'] = []\n",
    "accs['heartbeat'] = []\n",
    "accs['combined br hb'] = []\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "accs['test id'] = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # leave out person out validation\n",
    "    for ident in range(n):\n",
    "\n",
    "        # print current iteration and time elapsed from start\n",
    "        print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "        ## ----- Data preparation:\n",
    "        # Split the data\n",
    "        train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "        validation_idents = [idents[ident]]\n",
    "        test_idents = [idents[ident-1]]\n",
    "        \n",
    "        # save test id to see which id yielded which accuracies\n",
    "        accs['test id'].append(test_idents[0])\n",
    "\n",
    "        # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "        xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hb(path, train_idents, seconds)\n",
    "        xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hb(path, validation_idents, seconds)\n",
    "        xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hb(path, test_idents, seconds)\n",
    "\n",
    "        # Scale data with standard scaler then MinMax scaler\n",
    "        # Raw Phase data:\n",
    "        xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted breathing data:\n",
    "        br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted Heartbeat data:\n",
    "        hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "        cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify without autoencoders:\n",
    "        # Phase classifier:\n",
    "        model = dense_classifier_base()\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['phase'].append(curr_acc)\n",
    "\n",
    "        # Breathing classifier:\n",
    "        model = dense_classifier_base()\n",
    "        sc, curr_acc, epoch_data = model_train(model, br_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               br_valid, y_valid, br_test, y_test)\n",
    "        accs['breathing'].append(curr_acc)\n",
    "\n",
    "        # Heartbeat classifier:\n",
    "        model = dense_classifier_base()\n",
    "        sc, curr_acc, epoch_data = model_train(model, hb_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               hb_valid, y_valid, hb_test, y_test)\n",
    "        accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "        # Combined classifier:\n",
    "        model = dense_classifier_base()\n",
    "        sc, curr_acc, epoch_data = model_train(model, cmb_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               cmb_valid, y_valid, cmb_test, y_test)\n",
    "        accs['combined br hb'].append(curr_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify with autoencoders:\n",
    "        # AE Training params\n",
    "        batch_size = 256\n",
    "        epochs = 100\n",
    "\n",
    "        # Undercomplete AE:\n",
    "        autoencoder, encoded = undercomplete_ae(xt_train, 40)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = dense_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "        # Sparse AE:\n",
    "        autoencoder, encoded = sparse_ae(xt_train, 40)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = dense_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['sparse'].append(curr_acc)\n",
    "\n",
    "        # Deep AE:\n",
    "        autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=40, dec_layers=[256,512])\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = dense_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['deep'].append(curr_acc)\n",
    "\n",
    "        # Contractive AE:\n",
    "        autoencoder, encoded = contractive_ae(xt_train, 40)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = dense_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['contractive'].append(curr_acc)\n",
    "\n",
    "# Print total time required to run this\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed!\", \"Time elapsed:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>breathing</th>\n",
       "      <th>heartbeat</th>\n",
       "      <th>combined br hb</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "      <th>test id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>3n2f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>2gu87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>iz2ps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
       "0  0.550000   0.550000   0.433333        0.691667       0.566667  0.591667   \n",
       "1  0.616667   0.708333   0.550000        0.558333       0.658333  0.583333   \n",
       "2  0.466667   0.333333   0.491667        0.425000       0.533333  0.641667   \n",
       "\n",
       "       deep  contractive test id  \n",
       "0  0.508333     0.558333   3n2f9  \n",
       "1  0.641667     0.516667   2gu87  \n",
       "2  0.633333     0.458333   iz2ps  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.46666667\n",
      "- max: 0.6166667\n",
      "- mean: 0.5444445\n",
      "- median: 0.55\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.33333334\n",
      "- max: 0.7083333\n",
      "- mean: 0.53055555\n",
      "- median: 0.55\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.43333334\n",
      "- max: 0.55\n",
      "- mean: 0.49166667\n",
      "- median: 0.49166667\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.425\n",
      "- max: 0.69166666\n",
      "- mean: 0.55833334\n",
      "- median: 0.55833334\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.53333336\n",
      "- max: 0.65833336\n",
      "- mean: 0.5861111\n",
      "- median: 0.56666666\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5833333\n",
      "- max: 0.64166665\n",
      "- mean: 0.60555553\n",
      "- median: 0.59166664\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5083333\n",
      "- max: 0.64166665\n",
      "- mean: 0.59444445\n",
      "- median: 0.6333333\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.45833334\n",
      "- max: 0.55833334\n",
      "- mean: 0.51111114\n",
      "- median: 0.51666665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM-based classifier  \n",
    "based on the original author's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize hyperparameters with talos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_classifier(model, params):\n",
    "\n",
    "    model = layers.Reshape((-1, 1), input_shape=(model.shape), name='reshape_cl') (model)\n",
    "\n",
    "    model = layers.Dropout(params['dropout'], name='dropout_cl1') (model)\n",
    "    \n",
    "    model = Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides'],\n",
    "                     name='conv1d_cl1') (model)\n",
    "    \n",
    "    model = MaxPooling1D(pool_size=params['pool_size'], name='maxpool_cl1') (model)\n",
    "    \n",
    "    model = Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides'],\n",
    "                     name='conv1d_cl2') (model)\n",
    "    \n",
    "    model = MaxPooling1D(pool_size=params['pool_size'], name='maxpool_cl2') (model)\n",
    "    \n",
    "    model = layers.Dropout(params['dropout'], name='dropout_cl2') (model)\n",
    "\n",
    "    model = LSTM(params['lstm_output_size'], activation='sigmoid', name='lstm_cl') (model)\n",
    "\n",
    "    model = Dense(1, activation=params['last_activation'], name='dense_cl') (model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_classifier_base(params):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides']))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=params['pool_size']))\n",
    "    model.add(Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides']))\n",
    "    model.add(MaxPooling1D(pool_size=params['pool_size']))\n",
    "\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(LSTM(params['lstm_output_size']))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(params['last_activation']))\n",
    "\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_phase = {\n",
    "    'kernel_size': 32,\n",
    "    'strides': 4,\n",
    "    'pool_size': 2,\n",
    "    'filters': 8,\n",
    "    'lstm_output_size': 236,\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'dropout': 0.09,\n",
    "    'activation': 'relu',\n",
    "    'optimizer': 'Nadam',\n",
    "    'last_activation': 'sigmoid'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_br_hb = {\n",
    "    'kernel_size': 2,\n",
    "    'strides': 1,\n",
    "    'pool_size': 1,\n",
    "    'filters': 2,\n",
    "    'lstm_output_size': 4,\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'dropout': 0.09,\n",
    "    'activation': 'relu',\n",
    "    'optimizer': 'Nadam',\n",
    "    'last_activation': 'sigmoid'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'kernel_size': 4,\n",
    "    'filters': 2,\n",
    "    'strides': 2,\n",
    "    'pool_size': 2,\n",
    "    'dropout': 0.09,\n",
    "    'optimizer': 'Nadam',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'activation': 'relu',\n",
    "    'last_activation': 'sigmoid',\n",
    "    'lstm_output_size': 256,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 3 ; time elapsed: 0:00:00.005986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-4f7efbba93b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mautoencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeep_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n\u001b[1;32m--> 129\u001b[1;33m                                                xt_valid, xt_valid, xt_test, xt_test)\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-303f01a19973>\u001b[0m in \u001b[0;36mmodel_train\u001b[1;34m(model, x_train, y_train, batch_size, epochs, x_valid, y_valid, x_test, y_test)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Train model with the given training, validation, and test set, with appropriate batch size and # epochs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mepoch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3824\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3825\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3826\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3827\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['LSTM'] = {}\n",
    "accs = accuracies['LSTM']\n",
    "accs['phase'] = []\n",
    "accs['breathing'] = []\n",
    "accs['heartbeat'] = []\n",
    "accs['combined br hb'] = []\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "accs['test id'] = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # leave out person out validation\n",
    "    for ident in range(n):\n",
    "\n",
    "        # print current iteration and time elapsed from start\n",
    "        print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "        ## ----- Data preparation:\n",
    "        # Split the data\n",
    "        train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "        validation_idents = [idents[ident]]\n",
    "        test_idents = [idents[ident-1]]\n",
    "\n",
    "        # save test id to see which id yielded which accuracies\n",
    "        accs['test id'].append(test_idents[0])\n",
    "        \n",
    "        # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "        xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hb(path, train_idents, seconds)\n",
    "        xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hb(path, validation_idents, seconds)\n",
    "        xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hb(path, test_idents, seconds)\n",
    "\n",
    "        # Scale data with standard scaler then MinMax scaler\n",
    "        # Raw Phase data:\n",
    "        xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted breathing data:\n",
    "        br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted Heartbeat data:\n",
    "        hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "        cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify without autoencoders:\n",
    "        # Phase classifier:\n",
    "        model = LSTM_classifier_base(params_phase)\n",
    "        # reshape data for the classifier\n",
    "        xtt_train = xt_train.reshape(-1, xt_train[0].shape[0], 1)\n",
    "        xtt_valid = xt_valid.reshape(-1, xt_valid[0].shape[0], 1)\n",
    "        xtt_test = xt_test.reshape(-1, xt_test[0].shape[0], 1)\n",
    "        # train and evaluate\n",
    "        sc, curr_acc, epoch_data = model_train(model, xtt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xtt_valid, y_valid, xtt_test, y_test)\n",
    "        accs['phase'].append(curr_acc)\n",
    "\n",
    "        # Breathing classifier:\n",
    "        model = LSTM_classifier_base(params_br_hb)\n",
    "        # reshape data for the classifier\n",
    "        brt_train = br_train.reshape(-1, br_train[0].shape[0], 1)\n",
    "        brt_valid = br_valid.reshape(-1, br_valid[0].shape[0], 1)\n",
    "        brt_test = br_test.reshape(-1, br_test[0].shape[0], 1)\n",
    "        # train and evaluate\n",
    "        sc, curr_acc, epoch_data = model_train(model, brt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               brt_valid, y_valid, brt_test, y_test)\n",
    "        accs['breathing'].append(curr_acc)\n",
    "\n",
    "        # Heartbeat classifier:\n",
    "        model = LSTM_classifier_base(params_br_hb)\n",
    "        # reshape data for the classifier\n",
    "        hbt_train = hb_train.reshape(-1, hb_train[0].shape[0], 1)\n",
    "        hbt_valid = hb_valid.reshape(-1, hb_valid[0].shape[0], 1)\n",
    "        hbt_test = hb_test.reshape(-1, hb_test[0].shape[0], 1)\n",
    "        # train and evaluate\n",
    "        sc, curr_acc, epoch_data = model_train(model, hbt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               hbt_valid, y_valid, hbt_test, y_test)\n",
    "        accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "        # Combined classifier:\n",
    "        model = LSTM_classifier_base(params_br_hb)\n",
    "        # reshape data for the classifier\n",
    "        cmbt_train = cmb_train.reshape(-1, cmb_train[0].shape[0], 1)\n",
    "        cmbt_valid = cmb_valid.reshape(-1, cmb_valid[0].shape[0], 1)\n",
    "        cmbt_test = cmb_test.reshape(-1, cmb_test[0].shape[0], 1)\n",
    "        # train and evaluate\n",
    "        sc, curr_acc, epoch_data = model_train(model, cmbt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               cmbt_valid, y_valid, cmbt_test, y_test)\n",
    "        accs['combined br hb'].append(curr_acc)\n",
    "\n",
    "        \n",
    "        \n",
    "        ## ----- Classify with autoencoders:\n",
    "        # AE Training params\n",
    "        batch_size = 256\n",
    "        epochs = 100\n",
    "\n",
    "        # undercomplete AE\n",
    "        autoencoder, encoded = undercomplete_ae(xt_train, 40)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = LSTM_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "        # sparse AE\n",
    "        autoencoder, encoded = sparse_ae(xt_train, 40)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = LSTM_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['sparse'].append(curr_acc)\n",
    "\n",
    "        # deep AE\n",
    "        autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=40, dec_layers=[256,512])\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = LSTM_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['deep'].append(curr_acc)\n",
    "\n",
    "        # contractive AE\n",
    "        autoencoder, encoded = contractive_ae(xt_train, 40)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = LSTM_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['contractive'].append(curr_acc)\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed!\", \"Time elapsed:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>breathing</th>\n",
       "      <th>heartbeat</th>\n",
       "      <th>combined br hb</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "      <th>test id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>3n2f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>2gu87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>iz2ps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
       "0  0.691667   0.608333   0.416667        0.525000       0.566667  0.575000   \n",
       "1  0.450000   0.633333   0.500000        0.366667       0.541667  0.616667   \n",
       "2  0.533333   0.316667   0.500000        0.425000       0.683333  0.516667   \n",
       "\n",
       "       deep  contractive test id  \n",
       "0  0.566667     0.558333   3n2f9  \n",
       "1  0.483333     0.566667   2gu87  \n",
       "2  0.558333     0.575000   iz2ps  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.45\n",
      "- max: 0.69166666\n",
      "- mean: 0.55833334\n",
      "- median: 0.53333336\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.31666666\n",
      "- max: 0.6333333\n",
      "- mean: 0.51944447\n",
      "- median: 0.60833335\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.41666666\n",
      "- max: 0.5\n",
      "- mean: 0.4722222\n",
      "- median: 0.5\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.36666667\n",
      "- max: 0.525\n",
      "- mean: 0.43888888\n",
      "- median: 0.425\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.5416667\n",
      "- max: 0.68333334\n",
      "- mean: 0.59722227\n",
      "- median: 0.56666666\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.51666665\n",
      "- max: 0.6166667\n",
      "- mean: 0.5694444\n",
      "- median: 0.575\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.48333332\n",
      "- max: 0.56666666\n",
      "- mean: 0.5361111\n",
      "- median: 0.55833334\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.55833334\n",
      "- max: 0.575\n",
      "- mean: 0.56666666\n",
      "- median: 0.56666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper loop function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper loop funciton for the sklearn and XGBoost classifiers\n",
    "def helper_loop(classifier_function, idents, n=5):\n",
    "    #returns a dictionary with accuracies\n",
    "\n",
    "    # set the variables in the dictionary\n",
    "    accs = {}\n",
    "    accs['phase'] = []\n",
    "    accs['breathing'] = []\n",
    "    accs['heartbeat'] = []\n",
    "    accs['combined br hb'] = []\n",
    "    accs['undercomplete'] = []\n",
    "    accs['sparse'] = []\n",
    "    accs['deep'] = []\n",
    "    accs['contractive'] = []\n",
    "    accs['test id'] = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        # leave out person out validation\n",
    "        for ident in range(n):\n",
    "\n",
    "            # print current iteration and time elapsed from start\n",
    "            print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "            ## ----- Data preparation:\n",
    "            # Split the data\n",
    "            train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "            validation_idents = [idents[ident]]\n",
    "            test_idents = [idents[ident-1]]\n",
    "\n",
    "            # save test id to see which id yielded which accuracies\n",
    "            accs['test id'].append(test_idents[0])\n",
    "\n",
    "            # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "            xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hb(path, train_idents, seconds)\n",
    "            xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hb(path, validation_idents, seconds)\n",
    "            xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hb(path, test_idents, seconds)\n",
    "\n",
    "            # change the y arrays to flat 1d arrays\n",
    "            y_train = y_train.ravel()\n",
    "            y_valid = y_valid.ravel()\n",
    "            y_test = y_test.ravel()\n",
    "            \n",
    "            # Scale data with standard scaler then MinMax scaler\n",
    "            # Raw Phase data:\n",
    "            xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "            # Hand extracted breathing data:\n",
    "            br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "            # Hand extracted Heartbeat data:\n",
    "            hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "            # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "            cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "\n",
    "\n",
    "\n",
    "            ## ----- Classify without autoencoders:\n",
    "            # Phase classifier:\n",
    "            model = classifier_function()\n",
    "            model.fit(xt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xt_test) == y_test) / y_test.shape[0]\n",
    "            accs['phase'].append(curr_acc)\n",
    "\n",
    "            # Breathing classifier:\n",
    "            base_model = classifier_function()\n",
    "            base_model.fit(br_train, y_train)\n",
    "            curr_acc = np.sum(base_model.predict(br_valid) == y_valid) / y_test.shape[0]\n",
    "            accs['breathing'].append(curr_acc)\n",
    "\n",
    "            # Heartbeat classifier:\n",
    "            base_model = classifier_function()\n",
    "            base_model.fit(hb_train, y_train)\n",
    "            curr_acc = np.sum(base_model.predict(hb_valid) == y_valid) / y_test.shape[0]\n",
    "            accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "            # Combined classifier:\n",
    "            base_model = classifier_function()\n",
    "            base_model.fit(cmb_train, y_train)\n",
    "            curr_acc = np.sum(base_model.predict(cmb_valid) == y_valid) / y_test.shape[0]\n",
    "            accs['combined br hb'].append(curr_acc)\n",
    "\n",
    "\n",
    "\n",
    "            ## ----- Classify with autoencoders:\n",
    "            # AE Training params\n",
    "            batch_size = 256\n",
    "            epochs = 100\n",
    "\n",
    "            # undercomplete AE\n",
    "            autoencoder, encoded = undercomplete_ae(xt_train, 40, encoded_as_model=True)\n",
    "            sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                                   xt_valid, xt_valid, xt_test, xt_test)\n",
    "            model = classifier_function()\n",
    "            xtt_train = encoded.predict(xt_train)\n",
    "            xtt_test = encoded.predict(xt_test)\n",
    "            model.fit(xtt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xtt_test) == y_test) / y_test.shape[0]\n",
    "            accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "            # sparse AE\n",
    "            autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "            sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                                   xt_valid, xt_valid, xt_test, xt_test)\n",
    "            model = classifier_function()\n",
    "            xtt_train = encoded.predict(xt_train)\n",
    "            xtt_test = encoded.predict(xt_test)\n",
    "            model.fit(xtt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xtt_test) == y_test) / y_test.shape[0]\n",
    "            accs['sparse'].append(curr_acc)\n",
    "\n",
    "            # deep AE\n",
    "            autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "            sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                                   xt_valid, xt_valid, xt_test, xt_test)\n",
    "            model = classifier_function()\n",
    "            xtt_train = encoded.predict(xt_train)\n",
    "            xtt_test = encoded.predict(xt_test)\n",
    "            model.fit(xtt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xtt_test) == y_test) / y_test.shape[0]\n",
    "            accs['deep'].append(curr_acc)\n",
    "\n",
    "            # contractive AE\n",
    "            autoencoder, encoded = contractive_ae(xt_train, 40, encoded_as_model=True)\n",
    "            sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                                   xt_valid, xt_valid, xt_test, xt_test)\n",
    "            model = classifier_function()\n",
    "            xtt_train = encoded.predict(xt_train)\n",
    "            xtt_test = encoded.predict(xt_test)\n",
    "            model.fit(xtt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xtt_test) == y_test) / y_test.shape[0]\n",
    "            accs['contractive'].append(curr_acc)\n",
    "\n",
    "    # Print total time required to run this\n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Completed!\", \"Time elapsed:\", elapsed_time)\n",
    "    \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def KNN_classifier():\n",
    "    model = KNeighborsClassifier(p=3, n_neighbors=7, metric='cosine')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 3 ; time elapsed: 0:00:00.007594\n",
      "iteration: 2 of 3 ; time elapsed: 0:03:18.464849\n",
      "iteration: 3 of 3 ; time elapsed: 0:06:47.224069\n",
      "Completed! Time elapsed: 0:10:17.814471\n"
     ]
    }
   ],
   "source": [
    "accs = helper_loop(KNN_classifier, idents, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies['kNN'] = accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>breathing</th>\n",
       "      <th>heartbeat</th>\n",
       "      <th>combined br hb</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "      <th>test id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3n2f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>2gu87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>iz2ps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
       "0  0.775000   0.416667   0.416667        0.566667       0.583333  0.716667   \n",
       "1  0.633333   0.583333   0.458333        0.558333       0.550000  0.525000   \n",
       "2  0.491667   0.500000   0.583333        0.550000       0.633333  0.625000   \n",
       "\n",
       "       deep  contractive test id  \n",
       "0  0.733333     0.666667   3n2f9  \n",
       "1  0.608333     0.483333   2gu87  \n",
       "2  0.616667     0.633333   iz2ps  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.49166666666666664\n",
      "- max: 0.775\n",
      "- mean: 0.6333333333333333\n",
      "- median: 0.6333333333333333\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.4166666666666667\n",
      "- max: 0.5833333333333334\n",
      "- mean: 0.5\n",
      "- median: 0.5\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.4166666666666667\n",
      "- max: 0.5833333333333334\n",
      "- mean: 0.48611111111111116\n",
      "- median: 0.4583333333333333\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.55\n",
      "- max: 0.5666666666666667\n",
      "- mean: 0.5583333333333333\n",
      "- median: 0.5583333333333333\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.55\n",
      "- max: 0.6333333333333333\n",
      "- mean: 0.5888888888888889\n",
      "- median: 0.5833333333333334\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.525\n",
      "- max: 0.7166666666666667\n",
      "- mean: 0.6222222222222222\n",
      "- median: 0.625\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.6083333333333333\n",
      "- max: 0.7333333333333333\n",
      "- mean: 0.6527777777777778\n",
      "- median: 0.6166666666666667\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.48333333333333334\n",
      "- max: 0.6666666666666666\n",
      "- mean: 0.5944444444444444\n",
      "- median: 0.6333333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def SVC_classifier():\n",
    "    model = SVC(kernel='rbf', C=1.5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 3 ; time elapsed: 0:00:00.006118\n",
      "iteration: 2 of 3 ; time elapsed: 0:03:35.575648\n",
      "iteration: 3 of 3 ; time elapsed: 0:07:14.893960\n",
      "Completed! Time elapsed: 0:11:14.471448\n"
     ]
    }
   ],
   "source": [
    "accs = helper_loop(SVC_classifier, idents, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies['SVC'] = accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>breathing</th>\n",
       "      <th>heartbeat</th>\n",
       "      <th>combined br hb</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "      <th>test id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>3n2f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>2gu87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>iz2ps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
       "0  0.566667   0.475000   0.491667        0.433333            0.5  0.508333   \n",
       "1  0.425000   0.366667   0.408333        0.466667            0.5  0.491667   \n",
       "2  0.475000   0.575000   0.508333        0.441667            0.5  0.500000   \n",
       "\n",
       "       deep  contractive test id  \n",
       "0  0.475000     0.533333   3n2f9  \n",
       "1  0.491667     0.550000   2gu87  \n",
       "2  0.516667     0.575000   iz2ps  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.425\n",
      "- max: 0.5666666666666667\n",
      "- mean: 0.48888888888888893\n",
      "- median: 0.475\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.36666666666666664\n",
      "- max: 0.575\n",
      "- mean: 0.47222222222222215\n",
      "- median: 0.475\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.4083333333333333\n",
      "- max: 0.5083333333333333\n",
      "- mean: 0.4694444444444444\n",
      "- median: 0.49166666666666664\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.43333333333333335\n",
      "- max: 0.4666666666666667\n",
      "- mean: 0.44722222222222224\n",
      "- median: 0.44166666666666665\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.5\n",
      "- mean: 0.5\n",
      "- median: 0.5\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.49166666666666664\n",
      "- max: 0.5083333333333333\n",
      "- mean: 0.5\n",
      "- median: 0.5\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.475\n",
      "- max: 0.5166666666666667\n",
      "- mean: 0.49444444444444446\n",
      "- median: 0.49166666666666664\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5333333333333333\n",
      "- max: 0.575\n",
      "- mean: 0.5527777777777778\n",
      "- median: 0.55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def random_forest_classifier():\n",
    "    model = RandomForestClassifier(n_estimators = 250,\n",
    "                                     min_samples_split = 10,\n",
    "                                     min_samples_leaf = 4,\n",
    "                                     max_features = 'auto',\n",
    "                                     max_depth = 90,\n",
    "                                     bootstrap = True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 3 ; time elapsed: 0:00:00.006981\n",
      "iteration: 2 of 3 ; time elapsed: 0:03:56.278379\n",
      "iteration: 3 of 3 ; time elapsed: 0:07:58.041324\n",
      "Completed! Time elapsed: 0:12:17.872026\n"
     ]
    }
   ],
   "source": [
    "accs = helper_loop(random_forest_classifier, idents, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies['random_forest'] = accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>breathing</th>\n",
       "      <th>heartbeat</th>\n",
       "      <th>combined br hb</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "      <th>test id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>3n2f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>2gu87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>iz2ps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
       "0  1.000000   0.616667   0.425000        0.625000       1.000000  0.991667   \n",
       "1  0.416667   0.483333   0.316667        0.466667       0.416667  0.491667   \n",
       "2  0.375000   0.558333   0.558333        0.500000       0.525000  0.575000   \n",
       "\n",
       "       deep  contractive test id  \n",
       "0  0.983333     0.991667   3n2f9  \n",
       "1  0.500000     0.391667   2gu87  \n",
       "2  0.583333     0.525000   iz2ps  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.375\n",
      "- max: 1.0\n",
      "- mean: 0.5972222222222222\n",
      "- median: 0.4166666666666667\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.48333333333333334\n",
      "- max: 0.6166666666666667\n",
      "- mean: 0.5527777777777778\n",
      "- median: 0.5583333333333333\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.31666666666666665\n",
      "- max: 0.5583333333333333\n",
      "- mean: 0.43333333333333335\n",
      "- median: 0.425\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.4666666666666667\n",
      "- max: 0.625\n",
      "- mean: 0.5305555555555556\n",
      "- median: 0.5\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.4166666666666667\n",
      "- max: 1.0\n",
      "- mean: 0.6472222222222223\n",
      "- median: 0.525\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.49166666666666664\n",
      "- max: 0.9916666666666667\n",
      "- mean: 0.6861111111111112\n",
      "- median: 0.575\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.9833333333333333\n",
      "- mean: 0.688888888888889\n",
      "- median: 0.5833333333333334\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.39166666666666666\n",
      "- max: 0.9916666666666667\n",
      "- mean: 0.6361111111111111\n",
      "- median: 0.525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "def naive_bayesian_classifier():\n",
    "    model = ComplementNB()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 3 ; time elapsed: 0:00:00.007977\n",
      "iteration: 2 of 3 ; time elapsed: 0:03:43.846404\n",
      "iteration: 3 of 3 ; time elapsed: 0:07:34.599746\n",
      "Completed! Time elapsed: 0:11:46.461805\n"
     ]
    }
   ],
   "source": [
    "accs = helper_loop(naive_bayesian_classifier, idents, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies['naive_bayesian'] = accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>breathing</th>\n",
       "      <th>heartbeat</th>\n",
       "      <th>combined br hb</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "      <th>test id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>3n2f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>2gu87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>iz2ps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
       "0  0.508333   0.600000   0.533333        0.616667       0.475000  0.566667   \n",
       "1  0.608333   0.366667   0.566667        0.333333       0.541667  0.641667   \n",
       "2  0.508333   0.300000   0.525000        0.325000       0.541667  0.583333   \n",
       "\n",
       "       deep  contractive test id  \n",
       "0  0.458333     0.633333   3n2f9  \n",
       "1  0.541667     0.516667   2gu87  \n",
       "2  0.508333     0.516667   iz2ps  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.5083333333333333\n",
      "- max: 0.6083333333333333\n",
      "- mean: 0.5416666666666666\n",
      "- median: 0.5083333333333333\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.3\n",
      "- max: 0.6\n",
      "- mean: 0.4222222222222222\n",
      "- median: 0.36666666666666664\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.525\n",
      "- max: 0.5666666666666667\n",
      "- mean: 0.5416666666666666\n",
      "- median: 0.5333333333333333\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.325\n",
      "- max: 0.6166666666666667\n",
      "- mean: 0.425\n",
      "- median: 0.3333333333333333\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.475\n",
      "- max: 0.5416666666666666\n",
      "- mean: 0.5194444444444444\n",
      "- median: 0.5416666666666666\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5666666666666667\n",
      "- max: 0.6416666666666667\n",
      "- mean: 0.5972222222222223\n",
      "- median: 0.5833333333333334\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.4583333333333333\n",
      "- max: 0.5416666666666666\n",
      "- mean: 0.5027777777777778\n",
      "- median: 0.5083333333333333\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5166666666666667\n",
      "- max: 0.6333333333333333\n",
      "- mean: 0.5555555555555555\n",
      "- median: 0.5166666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGBoost_classifier():\n",
    "    model = XGBClassifier(n_estimators = 83)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 3 ; time elapsed: 0:00:00.009973\n",
      "iteration: 2 of 3 ; time elapsed: 0:04:15.784391\n",
      "iteration: 3 of 3 ; time elapsed: 0:08:29.976965\n",
      "Completed! Time elapsed: 0:13:07.088634\n"
     ]
    }
   ],
   "source": [
    "accs = helper_loop(XGBoost_classifier, idents, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies['XGBoost'] = accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>breathing</th>\n",
       "      <th>heartbeat</th>\n",
       "      <th>combined br hb</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "      <th>test id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3n2f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>2gu87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.425</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>iz2ps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
       "0  1.000   0.583333   0.450000        0.433333       1.000000  1.000000   \n",
       "1  0.450   0.516667   0.350000        0.433333       0.408333  0.433333   \n",
       "2  0.425   0.516667   0.541667        0.483333       0.508333  0.541667   \n",
       "\n",
       "       deep  contractive test id  \n",
       "0  0.991667     1.000000   3n2f9  \n",
       "1  0.533333     0.416667   2gu87  \n",
       "2  0.550000     0.533333   iz2ps  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.425\n",
      "- max: 1.0\n",
      "- mean: 0.625\n",
      "- median: 0.45\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.5166666666666667\n",
      "- max: 0.5833333333333334\n",
      "- mean: 0.5388888888888889\n",
      "- median: 0.5166666666666667\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.35\n",
      "- max: 0.5416666666666666\n",
      "- mean: 0.44722222222222224\n",
      "- median: 0.45\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.43333333333333335\n",
      "- max: 0.48333333333333334\n",
      "- mean: 0.45\n",
      "- median: 0.43333333333333335\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.4083333333333333\n",
      "- max: 1.0\n",
      "- mean: 0.6388888888888888\n",
      "- median: 0.5083333333333333\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.43333333333333335\n",
      "- max: 1.0\n",
      "- mean: 0.6583333333333333\n",
      "- median: 0.5416666666666666\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5333333333333333\n",
      "- max: 0.9916666666666667\n",
      "- mean: 0.6916666666666668\n",
      "- median: 0.55\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.4166666666666667\n",
      "- max: 1.0\n",
      "- mean: 0.65\n",
      "- median: 0.5333333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Compare Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print min, max, mean, median for each clasifier/autoencoder combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- simple_dense: -----------\n",
      "phase accuracies:\n",
      "- min: 0.46666667\n",
      "- max: 0.6166667\n",
      "- mean: 0.5444445\n",
      "- median: 0.55\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.33333334\n",
      "- max: 0.7083333\n",
      "- mean: 0.53055555\n",
      "- median: 0.55\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.43333334\n",
      "- max: 0.55\n",
      "- mean: 0.49166667\n",
      "- median: 0.49166667\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.425\n",
      "- max: 0.69166666\n",
      "- mean: 0.55833334\n",
      "- median: 0.55833334\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.53333336\n",
      "- max: 0.65833336\n",
      "- mean: 0.5861111\n",
      "- median: 0.56666666\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5833333\n",
      "- max: 0.64166665\n",
      "- mean: 0.60555553\n",
      "- median: 0.59166664\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5083333\n",
      "- max: 0.64166665\n",
      "- mean: 0.59444445\n",
      "- median: 0.6333333\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.45833334\n",
      "- max: 0.55833334\n",
      "- mean: 0.51111114\n",
      "- median: 0.51666665\n",
      "\n",
      "\n",
      "\n",
      "----------- LSTM: -----------\n",
      "phase accuracies:\n",
      "- min: 0.45\n",
      "- max: 0.69166666\n",
      "- mean: 0.55833334\n",
      "- median: 0.53333336\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.31666666\n",
      "- max: 0.6333333\n",
      "- mean: 0.51944447\n",
      "- median: 0.60833335\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.41666666\n",
      "- max: 0.5\n",
      "- mean: 0.4722222\n",
      "- median: 0.5\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.36666667\n",
      "- max: 0.525\n",
      "- mean: 0.43888888\n",
      "- median: 0.425\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.5416667\n",
      "- max: 0.68333334\n",
      "- mean: 0.59722227\n",
      "- median: 0.56666666\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.51666665\n",
      "- max: 0.6166667\n",
      "- mean: 0.5694444\n",
      "- median: 0.575\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.48333332\n",
      "- max: 0.56666666\n",
      "- mean: 0.5361111\n",
      "- median: 0.55833334\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.55833334\n",
      "- max: 0.575\n",
      "- mean: 0.56666666\n",
      "- median: 0.56666666\n",
      "\n",
      "\n",
      "\n",
      "----------- kNN: -----------\n",
      "phase accuracies:\n",
      "- min: 0.49166666666666664\n",
      "- max: 0.775\n",
      "- mean: 0.6333333333333333\n",
      "- median: 0.6333333333333333\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.4166666666666667\n",
      "- max: 0.5833333333333334\n",
      "- mean: 0.5\n",
      "- median: 0.5\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.4166666666666667\n",
      "- max: 0.5833333333333334\n",
      "- mean: 0.48611111111111116\n",
      "- median: 0.4583333333333333\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.55\n",
      "- max: 0.5666666666666667\n",
      "- mean: 0.5583333333333333\n",
      "- median: 0.5583333333333333\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.55\n",
      "- max: 0.6333333333333333\n",
      "- mean: 0.5888888888888889\n",
      "- median: 0.5833333333333334\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.525\n",
      "- max: 0.7166666666666667\n",
      "- mean: 0.6222222222222222\n",
      "- median: 0.625\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.6083333333333333\n",
      "- max: 0.7333333333333333\n",
      "- mean: 0.6527777777777778\n",
      "- median: 0.6166666666666667\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.48333333333333334\n",
      "- max: 0.6666666666666666\n",
      "- mean: 0.5944444444444444\n",
      "- median: 0.6333333333333333\n",
      "\n",
      "\n",
      "\n",
      "----------- SVC: -----------\n",
      "phase accuracies:\n",
      "- min: 0.425\n",
      "- max: 0.5666666666666667\n",
      "- mean: 0.48888888888888893\n",
      "- median: 0.475\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.36666666666666664\n",
      "- max: 0.575\n",
      "- mean: 0.47222222222222215\n",
      "- median: 0.475\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.4083333333333333\n",
      "- max: 0.5083333333333333\n",
      "- mean: 0.4694444444444444\n",
      "- median: 0.49166666666666664\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.43333333333333335\n",
      "- max: 0.4666666666666667\n",
      "- mean: 0.44722222222222224\n",
      "- median: 0.44166666666666665\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.5\n",
      "- mean: 0.5\n",
      "- median: 0.5\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.49166666666666664\n",
      "- max: 0.5083333333333333\n",
      "- mean: 0.5\n",
      "- median: 0.5\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.475\n",
      "- max: 0.5166666666666667\n",
      "- mean: 0.49444444444444446\n",
      "- median: 0.49166666666666664\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5333333333333333\n",
      "- max: 0.575\n",
      "- mean: 0.5527777777777778\n",
      "- median: 0.55\n",
      "\n",
      "\n",
      "\n",
      "----------- random_forest: -----------\n",
      "phase accuracies:\n",
      "- min: 0.375\n",
      "- max: 1.0\n",
      "- mean: 0.5972222222222222\n",
      "- median: 0.4166666666666667\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.48333333333333334\n",
      "- max: 0.6166666666666667\n",
      "- mean: 0.5527777777777778\n",
      "- median: 0.5583333333333333\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.31666666666666665\n",
      "- max: 0.5583333333333333\n",
      "- mean: 0.43333333333333335\n",
      "- median: 0.425\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.4666666666666667\n",
      "- max: 0.625\n",
      "- mean: 0.5305555555555556\n",
      "- median: 0.5\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.4166666666666667\n",
      "- max: 1.0\n",
      "- mean: 0.6472222222222223\n",
      "- median: 0.525\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.49166666666666664\n",
      "- max: 0.9916666666666667\n",
      "- mean: 0.6861111111111112\n",
      "- median: 0.575\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.9833333333333333\n",
      "- mean: 0.688888888888889\n",
      "- median: 0.5833333333333334\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.39166666666666666\n",
      "- max: 0.9916666666666667\n",
      "- mean: 0.6361111111111111\n",
      "- median: 0.525\n",
      "\n",
      "\n",
      "\n",
      "----------- naive_bayesian: -----------\n",
      "phase accuracies:\n",
      "- min: 0.5083333333333333\n",
      "- max: 0.6083333333333333\n",
      "- mean: 0.5416666666666666\n",
      "- median: 0.5083333333333333\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.3\n",
      "- max: 0.6\n",
      "- mean: 0.4222222222222222\n",
      "- median: 0.36666666666666664\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.525\n",
      "- max: 0.5666666666666667\n",
      "- mean: 0.5416666666666666\n",
      "- median: 0.5333333333333333\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.325\n",
      "- max: 0.6166666666666667\n",
      "- mean: 0.425\n",
      "- median: 0.3333333333333333\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.475\n",
      "- max: 0.5416666666666666\n",
      "- mean: 0.5194444444444444\n",
      "- median: 0.5416666666666666\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5666666666666667\n",
      "- max: 0.6416666666666667\n",
      "- mean: 0.5972222222222223\n",
      "- median: 0.5833333333333334\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.4583333333333333\n",
      "- max: 0.5416666666666666\n",
      "- mean: 0.5027777777777778\n",
      "- median: 0.5083333333333333\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5166666666666667\n",
      "- max: 0.6333333333333333\n",
      "- mean: 0.5555555555555555\n",
      "- median: 0.5166666666666667\n",
      "\n",
      "\n",
      "\n",
      "----------- XGBoost: -----------\n",
      "phase accuracies:\n",
      "- min: 0.425\n",
      "- max: 1.0\n",
      "- mean: 0.625\n",
      "- median: 0.45\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.5166666666666667\n",
      "- max: 0.5833333333333334\n",
      "- mean: 0.5388888888888889\n",
      "- median: 0.5166666666666667\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.35\n",
      "- max: 0.5416666666666666\n",
      "- mean: 0.44722222222222224\n",
      "- median: 0.45\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.43333333333333335\n",
      "- max: 0.48333333333333334\n",
      "- mean: 0.45\n",
      "- median: 0.43333333333333335\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.4083333333333333\n",
      "- max: 1.0\n",
      "- mean: 0.6388888888888888\n",
      "- median: 0.5083333333333333\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.43333333333333335\n",
      "- max: 1.0\n",
      "- mean: 0.6583333333333333\n",
      "- median: 0.5416666666666666\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5333333333333333\n",
      "- max: 0.9916666666666667\n",
      "- mean: 0.6916666666666668\n",
      "- median: 0.55\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.4166666666666667\n",
      "- max: 1.0\n",
      "- mean: 0.65\n",
      "- median: 0.5333333333333333\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in accuracies:\n",
    "    print(\"-----------\", classifier + \":\", \"-----------\")\n",
    "    accs = accuracies[classifier]\n",
    "    print_accs_stats(accs)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all accuracies in table form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_dense:\n",
      "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
      "0  0.550000   0.550000   0.433333        0.691667       0.566667  0.591667   \n",
      "1  0.616667   0.708333   0.550000        0.558333       0.658333  0.583333   \n",
      "2  0.466667   0.333333   0.491667        0.425000       0.533333  0.641667   \n",
      "\n",
      "       deep  contractive test id  \n",
      "0  0.508333     0.558333   3n2f9  \n",
      "1  0.641667     0.516667   2gu87  \n",
      "2  0.633333     0.458333   iz2ps  \n",
      "\n",
      "\n",
      "LSTM:\n",
      "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
      "0  0.691667   0.608333   0.416667        0.525000       0.566667  0.575000   \n",
      "1  0.450000   0.633333   0.500000        0.366667       0.541667  0.616667   \n",
      "2  0.533333   0.316667   0.500000        0.425000       0.683333  0.516667   \n",
      "\n",
      "       deep  contractive test id  \n",
      "0  0.566667     0.558333   3n2f9  \n",
      "1  0.483333     0.566667   2gu87  \n",
      "2  0.558333     0.575000   iz2ps  \n",
      "\n",
      "\n",
      "kNN:\n",
      "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
      "0  0.775000   0.416667   0.416667        0.566667       0.583333  0.716667   \n",
      "1  0.633333   0.583333   0.458333        0.558333       0.550000  0.525000   \n",
      "2  0.491667   0.500000   0.583333        0.550000       0.633333  0.625000   \n",
      "\n",
      "       deep  contractive test id  \n",
      "0  0.733333     0.666667   3n2f9  \n",
      "1  0.608333     0.483333   2gu87  \n",
      "2  0.616667     0.633333   iz2ps  \n",
      "\n",
      "\n",
      "SVC:\n",
      "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
      "0  0.566667   0.475000   0.491667        0.433333            0.5  0.508333   \n",
      "1  0.425000   0.366667   0.408333        0.466667            0.5  0.491667   \n",
      "2  0.475000   0.575000   0.508333        0.441667            0.5  0.500000   \n",
      "\n",
      "       deep  contractive test id  \n",
      "0  0.475000     0.533333   3n2f9  \n",
      "1  0.491667     0.550000   2gu87  \n",
      "2  0.516667     0.575000   iz2ps  \n",
      "\n",
      "\n",
      "random_forest:\n",
      "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
      "0  1.000000   0.616667   0.425000        0.625000       1.000000  0.991667   \n",
      "1  0.416667   0.483333   0.316667        0.466667       0.416667  0.491667   \n",
      "2  0.375000   0.558333   0.558333        0.500000       0.525000  0.575000   \n",
      "\n",
      "       deep  contractive test id  \n",
      "0  0.983333     0.991667   3n2f9  \n",
      "1  0.500000     0.391667   2gu87  \n",
      "2  0.583333     0.525000   iz2ps  \n",
      "\n",
      "\n",
      "naive_bayesian:\n",
      "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
      "0  0.508333   0.600000   0.533333        0.616667       0.475000  0.566667   \n",
      "1  0.608333   0.366667   0.566667        0.333333       0.541667  0.641667   \n",
      "2  0.508333   0.300000   0.525000        0.325000       0.541667  0.583333   \n",
      "\n",
      "       deep  contractive test id  \n",
      "0  0.458333     0.633333   3n2f9  \n",
      "1  0.541667     0.516667   2gu87  \n",
      "2  0.508333     0.516667   iz2ps  \n",
      "\n",
      "\n",
      "XGBoost:\n",
      "   phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
      "0  1.000   0.583333   0.450000        0.433333       1.000000  1.000000   \n",
      "1  0.450   0.516667   0.350000        0.433333       0.408333  0.433333   \n",
      "2  0.425   0.516667   0.541667        0.483333       0.508333  0.541667   \n",
      "\n",
      "       deep  contractive test id  \n",
      "0  0.991667     1.000000   3n2f9  \n",
      "1  0.533333     0.416667   2gu87  \n",
      "2  0.550000     0.533333   iz2ps  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in accuracies:\n",
    "    print(classifier + \":\")\n",
    "    print(pandas.DataFrame.from_dict(accuracies[classifier]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37talos",
   "language": "python",
   "name": "py37talos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
