{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "Exploring different classifiers with different autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoders:  \n",
    "[Undercomplete Autoencoder](#Undercomplete-Autoencoder)  \n",
    "[Sparse Autoencoder](#Sparse-Autoencoder)  \n",
    "[Deep Autoencoder](#Deep-Autoencoder)  \n",
    "[Contractive Autoencoder](#Contractive-Autoencoder)  \n",
    "\n",
    "classifiers:  \n",
    "[Simple dense layer](#Simple-dense-layer)  \n",
    "[LSTM-based classifier](#LSTM-based-classifier)  \n",
    "[kNN](#kNN)  \n",
    "[SVC](#SVC)  \n",
    "[Random Forest](#Random-Forest)  \n",
    "[XGBoost](#XGBoost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datareader # made by the previous author for reading the collected data\n",
    "import dataextractor # same as above\n",
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# need to disable eager execution for .get_weights() in contractive autoencoder loss to work\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "# required for the contractive autoencoder\n",
    "import tensorflow.keras.backend as K\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import talos\n",
    "from talos.utils import lr_normalizer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.keras.backend.set_floatx('float32') # call this, to set keras to use float32 to avoid a warning message\n",
    "metrics = ['accuracy',\n",
    "           keras.metrics.TruePositives(),\n",
    "           keras.metrics.FalsePositives(),\n",
    "           keras.metrics.TrueNegatives(),\n",
    "           keras.metrics.FalseNegatives()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original author's code, just copied into separate cells of this jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_busy_vs_relax_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data from either 'on task' or 'relax' time frames and their class (0 or 1).\n",
    "    TODO: join functions\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### task versus relax (1 sample each)\n",
    "        dataextract = dataextractor.DataExtractor(data[0][busy_n[task_num_table][0]:busy_n[task_num_table][1]],\n",
    "                                                  data[1][busy_n[task_num_table][0]:busy_n[task_num_table][1]],\n",
    "                                                  samp_rate)\n",
    "\n",
    "        dataextract_relax = dataextractor.DataExtractor(data[0][relax_n[task_num_table][0]:relax_n[task_num_table][1]],\n",
    "                                                        data[1][relax_n[task_num_table][0]:relax_n[task_num_table][1]],\n",
    "                                                        samp_rate)\n",
    "        try:\n",
    "            tasks_data = np.vstack((tasks_data, dataextract.y[-samp_rate * seconds:]))\n",
    "            tasks_y = np.vstack((tasks_y, 1))\n",
    "            tasks_data = np.vstack((tasks_data, dataextract_relax.y[-samp_rate * seconds:]))\n",
    "            tasks_y = np.vstack((tasks_y, 0))\n",
    "        except ValueError:\n",
    "            continue\n",
    "#             print(ident)  # ignore short windows\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engagement_increase_vs_decrease_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data from either engagement 'increase' or 'decrease' time frames and their class (0 or 1).\n",
    "    TODO: join functions\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds * samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### engagement increase / decrease\n",
    "        if task_num_table == 0:\n",
    "            continue\n",
    "        mid = int((relax_n[task_num_table][0] + relax_n[task_num_table][1])/2)\n",
    "        length = int(samp_rate*30)\n",
    "        for j in range(10):\n",
    "            new_end = int(mid-j*samp_rate)\n",
    "\n",
    "            new_start2 = int(mid+j*samp_rate)\n",
    "\n",
    "            dataextract_decrease = dataextractor.DataExtractor(data[0][new_end - length:new_end],\n",
    "                                                               data[1][new_end-length:new_end],\n",
    "                                                               samp_rate)\n",
    "\n",
    "            dataextract_increase = dataextractor.DataExtractor(data[0][new_start2: new_start2 + length],\n",
    "                                                               data[1][new_start2: new_start2 + length], samp_rate)\n",
    "\n",
    "            try:\n",
    "                tasks_data = np.vstack((tasks_data, dataextract_increase.y))\n",
    "                tasks_y = np.vstack((tasks_y, 1))\n",
    "                tasks_data = np.vstack((tasks_data, dataextract_decrease.y))\n",
    "                tasks_y = np.vstack((tasks_y, 0))\n",
    "            except ValueError:\n",
    "                print(ident)  # ignore short windows\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_complexities_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data along with task complexity class.\n",
    "    TODO: join functions. Add parameter to choose different task types and complexities\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### task complexity classification\n",
    "        if cog_res['task_complexity'][task_num_table] == 'medium':\n",
    "            continue\n",
    "        # if cog_res['task_label'][task_num_table] == 'FA' or cog_res['task_label'][task_num_table] == 'HP':\n",
    "        #     continue\n",
    "        if cog_res['task_label'][task_num_table] != 'NC':\n",
    "            continue\n",
    "        map_compl = {\n",
    "            'low': 0,\n",
    "            'medium': 2,\n",
    "            'high': 1\n",
    "        }\n",
    "        for j in range(10):\n",
    "            new_end = int(busy_n[task_num_table][1] - j * samp_rate)\n",
    "            new_start = int(new_end - samp_rate*30)\n",
    "            dataextract = dataextractor.DataExtractor(data[0][new_start:new_end],\n",
    "                                                      data[1][new_start:new_end], samp_rate)\n",
    "            try:\n",
    "                tasks_data = np.vstack((tasks_data, dataextract.y))\n",
    "                tasks_y = np.vstack((tasks_y, map_compl.get(cog_res['task_complexity'][task_num_table])))\n",
    "            except ValueError:\n",
    "                print(ident)\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TLX_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data along with task load index class.\n",
    "    TODO: join functions. Add parameter to choose different task types and complexities\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### task load index\n",
    "        if cog_res['task_complexity'][task_num_table] == 'medium' or cog_res['task_label'][task_num_table] != 'PT':\n",
    "            continue\n",
    "        for j in range(10):\n",
    "            new_end = int(busy_n[task_num_table][1] - j * samp_rate)\n",
    "            new_start = int(new_end - samp_rate*30)\n",
    "            dataextract = dataextractor.DataExtractor(data[0][new_start:new_end],\n",
    "                                                      data[1][new_start:new_end], samp_rate)\n",
    "            try:\n",
    "                tasks_data = np.vstack((tasks_data, dataextract.y))\n",
    "                tasks_y = np.vstack((tasks_y, cog_res['task_load_index'][task_num_table]))\n",
    "            except ValueError:\n",
    "                print(ident)\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_idents(path, idents, seconds):\n",
    "    \"\"\"Go through all user data and take out windows of only <seconds> long time frames,\n",
    "    along with the given class (from 'divide_each_task' function).\n",
    "    \"\"\"\n",
    "    samp_rate = 43  # hard-coded sample rate\n",
    "    data, ys = np.empty((0, samp_rate*seconds)), np.empty((0, 1))\n",
    "    for i in idents:\n",
    "        x, y = get_busy_vs_relax_timeframes(path, i, seconds) # either 'get_busy_vs_relax_timeframes',\n",
    "        # get_engagement_increase_vs_decrease_timeframes, get_task_complexities_timeframes or get_TLX_timeframes\n",
    "        # TODO: ^ modify, so that different functions can be accessible by parameter\n",
    "        data = np.vstack((data, x))\n",
    "        ys = np.vstack((ys, y))\n",
    "    return data, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, x_train, y_train, batch_size, epochs, x_valid, y_valid, x_test, y_test):\n",
    "    \"\"\"Train model with the given training, validation, and test set, with appropriate batch size and # epochs.\"\"\"\n",
    "    epoch_data = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_valid, y_valid), verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    acc = score[1]\n",
    "    score = score[0]\n",
    "    return score, acc, epoch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(x, maxlen):\n",
    "    \"\"\"Pad sequences (all have to be same length).\"\"\"\n",
    "    print('Pad sequences (samples x time)')\n",
    "    return sequence.pad_sequences(x, maxlen=maxlen, dtype=np.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undercomplete Autoencoder  \n",
    "from https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undercomplete_ae(x, encoding_dim=64, encoded_as_model=False):\n",
    "    # Simplest possible autoencoder from https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_data = Input(shape=x[0].shape, name=\"input\")\n",
    "    dropout = Dropout(0.25, name=\"dropout\")(input_data)\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation='relu', name=\"encoded\")(dropout)\n",
    "    \n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(x[0].shape[0], activation='sigmoid', name=\"decoded\")(encoded)\n",
    "\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "    \n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Autoencoder  \n",
    "from https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_ae(x, encoding_dim=64, encoded_as_model=False):\n",
    "    # Simplest possible autoencoder from https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_data = Input(shape=x[0].shape, name=\"input\")\n",
    "    dropout = Dropout(0.25, name=\"dropout\") (input_data)\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    # add a sparsity constraint\n",
    "    encoded = Dense(encoding_dim, activation='relu', name=\"encoded\",\n",
    "                    activity_regularizer=regularizers.l1(10e-5))(dropout)\n",
    "    \n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(x[0].shape[0], activation='sigmoid', name=\"decoded\")(encoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_data, decoded, name=\"sparse_ae\")\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "    \n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Autoencoder  \n",
    "from https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_ae(x, enc_layers=[512,256], encoding_dim=64, dec_layers=[256,512], encoded_as_model=False):\n",
    "    # From https://www.tensorflow.org/guide/keras/functional#use_the_same_graph_of_layers_to_define_multiple_models\n",
    "    input_data = keras.Input(shape=x[0].shape, name=\"normalized_signal\")\n",
    "    model = Dropout(0.25, name=\"dropout\", autocast=False)(input_data)\n",
    "    for i in enumerate(enc_layers):\n",
    "        model = Dense(i[1], activation=\"relu\", name=\"dense_enc_\" + str(i[0]+1))(model)\n",
    "    encoded_output = Dense(encoding_dim, activation=\"relu\", name=\"encoded_signal\")(model)\n",
    "\n",
    "    encoded = encoded_output\n",
    "\n",
    "    model = layers.Dense(dec_layers[0], activation=\"sigmoid\", name=\"dense_dec_1\")(encoded_output)\n",
    "    for i in enumerate(dec_layers[1:]):\n",
    "        model = Dense(i[1], activation=\"sigmoid\", name=\"dense_dec_\" + str(i[0]+2))(model)\n",
    "    decoded_output = Dense(x[0].shape[0], activation=\"sigmoid\", name=\"reconstructed_signal\")(model)\n",
    "    \n",
    "    autoencoder = Model(input_data, decoded_output, name=\"autoencoder\")\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "\n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contractive Autoencoder\n",
    "From: https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to be able to access the autoencoder in the loss funciton\n",
    "def loss_with_params(autoencoder):\n",
    "    # loss function from https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/\n",
    "    def contractive_loss(y_pred, y_true):\n",
    "\n",
    "        lam = 1e-4\n",
    "        mse = K.mean(K.square(y_true - y_pred), axis=1)\n",
    "\n",
    "        W = K.variable(value=autoencoder.get_layer('encoded').get_weights()[0])  # N x N_hidden\n",
    "        W = K.transpose(W)  # N_hidden x N\n",
    "        h = autoencoder.get_layer('encoded').output\n",
    "        dh = h * (1 - h)  # N_batch x N_hidden\n",
    "\n",
    "        # N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n",
    "        contractive = lam * K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n",
    "\n",
    "        return mse + contractive\n",
    "    return contractive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractive_ae(x, encoding_dim=64, encoded_as_model=False):\n",
    "    # From https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/\n",
    "\n",
    "    input_data = Input(shape=x[0].shape, name=\"input\")\n",
    "    encoded = Dense(encoding_dim, activation='sigmoid', name='encoded')(input_data)\n",
    "    outputs = Dense(x[0].shape[0], activation='linear', name=\"output\")(encoded)\n",
    "\n",
    "    autoencoder = Model(input_data, outputs, name=\"autoencoder\")\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss=loss_with_params(autoencoder), metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "    \n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary to store accuracies for comparison\n",
    "accuracies = {}\n",
    "\n",
    "# used for reading the data into an array\n",
    "seconds = 30  # time window length\n",
    "idents = ['2gu87', 'iz2ps', '1mpau', '7dwjy', '7swyk', '94mnx', 'bd47a', 'c24ur', 'ctsax', 'dkhty', 'e4gay',\n",
    "              'ef5rq', 'f1gjp', 'hpbxa', 'pmyfl', 'r89k1', 'tn4vl', 'td5pr', 'gyqu9', 'fzchw', 'l53hg', '3n2f9',\n",
    "              '62i9y']\n",
    "path = '../../../StudyData/'\n",
    "\n",
    "# change to len(idents) at the end to use all the data\n",
    "n = 5 #len(idents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_classifier(model, params):\n",
    "    \n",
    "    model = Dropout(params['dropout'], name='dropout_cl')(model)\n",
    "    model = Dense(params['hidden_size'], activation=params['activation'], name='dense_cl1')(model)\n",
    "    model = Dense(1, activation=params['last_activation'], name='dense_cl2')(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dropout': 0.24,\n",
    "    'optimizer': 'Adam',\n",
    "    'hidden_size': 64,\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'last_activation': 'sigmoid',\n",
    "    'activation': 'softmax',\n",
    "    'batch_size': 256,\n",
    "    'epochs': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5\n",
      "WARNING:tensorflow:From D:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "iteration: 2 of 5\n",
      "iteration: 3 of 5\n",
      "iteration: 4 of 5\n",
      "iteration: 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['simple_dense'] = {}\n",
    "accs = accuracies['simple_dense']\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "\n",
    "# leave out person out validation\n",
    "for ident in range(n):\n",
    "    \n",
    "    print(\"iteration:\", ident+1, \"of\", n)\n",
    "    \n",
    "    train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "    validation_idents = [idents[ident]]\n",
    "    test_idents = [idents[ident-1]]\n",
    "\n",
    "    # Load data\n",
    "    xt_train, y_train = get_data_from_idents(path, train_idents, seconds)\n",
    "    xt_valid, y_valid = get_data_from_idents(path, validation_idents, seconds)\n",
    "    xt_test, y_test = get_data_from_idents(path, test_idents, seconds)\n",
    "\n",
    "    # Scale with standard scaler\n",
    "    sscaler = StandardScaler()\n",
    "    sscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = sscaler.transform(xt_train)\n",
    "    xt_valid = sscaler.transform(xt_valid)\n",
    "    xt_test = sscaler.transform(xt_test)\n",
    "\n",
    "    # Scale with MinMax to range [0,1]\n",
    "    mmscaler = MinMaxScaler()\n",
    "    mmscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = mmscaler.transform(xt_train)\n",
    "    xt_valid = mmscaler.transform(xt_valid)\n",
    "    xt_test = mmscaler.transform(xt_test)\n",
    "\n",
    "    # AE Training params\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "\n",
    "    # undercomplete AE\n",
    "    autoencoder, encoded = undercomplete_ae(xt_train, 60)\n",
    "#     print(\"undercomplete AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = dense_classifier(encoded, params)\n",
    "    model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=metrics)\n",
    "#     print(\"undercomplete CLASSIFICATION TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                           xt_valid, y_valid, xt_test, y_test)\n",
    "    accs['undercomplete'].append(curr_acc)\n",
    "    \n",
    "    # sparse AE\n",
    "    autoencoder, encoded = sparse_ae(xt_train, 60)\n",
    "#     print(\"sparse AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = dense_classifier(encoded, params)\n",
    "    model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=metrics)\n",
    "#     print(\"sparse CLASSIFICATION TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                           xt_valid, y_valid, xt_test, y_test)\n",
    "    accs['sparse'].append(curr_acc)\n",
    "\n",
    "    # deep AE\n",
    "    autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512])\n",
    "#     print(\"deep AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = dense_classifier(encoded, params)\n",
    "    model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=metrics)\n",
    "#     print(\"deep CLASSIFICATION TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                           xt_valid, y_valid, xt_test, y_test)\n",
    "    accs['deep'].append(curr_acc)\n",
    "    \n",
    "    # contractive AE\n",
    "    autoencoder, encoded = contractive_ae(xt_train, 60)\n",
    "#     print(\"contractive AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = dense_classifier(encoded, params)\n",
    "    model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=metrics)\n",
    "#     print(\"contractive CLASSIFICATION TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                           xt_valid, y_valid, xt_test, y_test)\n",
    "    accs['contractive'].append(curr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   undercomplete    sparse      deep  contractive\n",
       "0       0.821429  0.821429  0.785714     0.785714\n",
       "1       0.735294  0.735294  0.676471     0.588235\n",
       "2       0.638889  0.638889  0.638889     0.472222\n",
       "3       0.558824  0.588235  0.588235     0.647059\n",
       "4       0.566667  0.600000  0.500000     0.566667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undercomplete accuracies:\n",
      "- min: 0.5588235\n",
      "- max: 0.8214286\n",
      "- mean: 0.66422033\n",
      "- median: 0.6388889\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5882353\n",
      "- max: 0.8214286\n",
      "- mean: 0.6767694\n",
      "- median: 0.6388889\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.78571427\n",
      "- mean: 0.63786185\n",
      "- median: 0.6388889\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.4722222\n",
      "- max: 0.78571427\n",
      "- mean: 0.6119794\n",
      "- median: 0.5882353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM-based classifier  \n",
    "based on the original author's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize hyperparameters with talos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_classifier(model, params):\n",
    "\n",
    "    model = layers.Reshape((-1, 1), input_shape=(model.shape), name='reshape_cl') (model)\n",
    "\n",
    "    model = layers.Dropout(params['dropout'], name='dropout_cl1') (model)\n",
    "    \n",
    "    model = Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides'],\n",
    "                     name='conv1d_cl1') (model)\n",
    "    \n",
    "    model = MaxPooling1D(pool_size=params['pool_size'], name='maxpool_cl1') (model)\n",
    "    \n",
    "    model = Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides'],\n",
    "                     name='conv1d_cl2') (model)\n",
    "    \n",
    "    model = MaxPooling1D(pool_size=params['pool_size'], name='maxpool_cl2') (model)\n",
    "    \n",
    "    model = layers.Dropout(params['dropout'], name='dropout_cl2') (model)\n",
    "\n",
    "    model = LSTM(params['lstm_output_size'], activation='sigmoid', name='lstm_cl') (model)\n",
    "\n",
    "    model = Dense(1, activation=params['last_activation'], name='dense_cl') (model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'kernel_size': 8,\n",
    "    'filters': 3,\n",
    "    'strides': 2,\n",
    "    'pool_size': 2,\n",
    "    'dropout': 0.09,\n",
    "    'optimizer': 'Nadam',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'activation': 'relu',\n",
    "    'last_activation': 'sigmoid',\n",
    "    'lstm_output_size': 256,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5\n",
      "iteration: 2 of 5\n",
      "iteration: 3 of 5\n",
      "iteration: 4 of 5\n",
      "iteration: 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['LSTM'] = {}\n",
    "accs = accuracies['LSTM']\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "\n",
    "# leave out person out validation\n",
    "for ident in range(n):\n",
    "    \n",
    "    print(\"iteration:\", ident+1, \"of\", n)\n",
    "    \n",
    "    train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "    validation_idents = [idents[ident]]\n",
    "    test_idents = [idents[ident-1]]\n",
    "\n",
    "    # Load data\n",
    "    xt_train, y_train = get_data_from_idents(path, train_idents, seconds)\n",
    "    xt_valid, y_valid = get_data_from_idents(path, validation_idents, seconds)\n",
    "    xt_test, y_test = get_data_from_idents(path, test_idents, seconds)\n",
    "\n",
    "    # Scale with standard scaler\n",
    "    sscaler = StandardScaler()\n",
    "    sscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = sscaler.transform(xt_train)\n",
    "    xt_valid = sscaler.transform(xt_valid)\n",
    "    xt_test = sscaler.transform(xt_test)\n",
    "\n",
    "    # Scale with MinMax to range [0,1]\n",
    "    mmscaler = MinMaxScaler()\n",
    "    mmscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = mmscaler.transform(xt_train)\n",
    "    xt_valid = mmscaler.transform(xt_valid)\n",
    "    xt_test = mmscaler.transform(xt_test)\n",
    "\n",
    "    # AE Training params\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "\n",
    "    # undercomplete AE\n",
    "    autoencoder, encoded = undercomplete_ae(xt_train, 60)\n",
    "#     print(\"undercomplete AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = LSTM_classifier(encoded, params)\n",
    "    model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=metrics)\n",
    "#     print(\"undercomplete CLASSIFICATION TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                           xt_valid, y_valid, xt_test, y_test)\n",
    "    accs['undercomplete'].append(curr_acc)\n",
    "    \n",
    "    # sparse AE\n",
    "    autoencoder, encoded = sparse_ae(xt_train, 60)\n",
    "#     print(\"sparse AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = LSTM_classifier(encoded, params)\n",
    "    model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=metrics)\n",
    "#     print(\"sparse CLASSIFICATION TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                           xt_valid, y_valid, xt_test, y_test)\n",
    "    accs['sparse'].append(curr_acc)\n",
    "\n",
    "    # deep AE\n",
    "    autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512])\n",
    "#     print(\"deep AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = LSTM_classifier(encoded, params)\n",
    "    model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=metrics)\n",
    "#     print(\"deep CLASSIFICATION TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                           xt_valid, y_valid, xt_test, y_test)\n",
    "    accs['deep'].append(curr_acc)\n",
    "    \n",
    "    # contractive AE\n",
    "    autoencoder, encoded = contractive_ae(xt_train, 60)\n",
    "#     print(\"contractive AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = LSTM_classifier(encoded, params)\n",
    "    model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=metrics)\n",
    "#     print(\"contractive CLASSIFICATION TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                           xt_valid, y_valid, xt_test, y_test)\n",
    "    accs['contractive'].append(curr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   undercomplete    sparse      deep  contractive\n",
       "0       0.785714  0.892857  0.857143     0.857143\n",
       "1       0.558824  0.735294  0.529412     0.764706\n",
       "2       0.694444  0.694444  0.694444     0.750000\n",
       "3       0.588235  0.529412  0.500000     0.588235\n",
       "4       0.666667  0.633333  0.500000     0.600000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undercomplete accuracies:\n",
      "- min: 0.5588235\n",
      "- max: 0.78571427\n",
      "- mean: 0.6587769\n",
      "- median: 0.6666667\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5294118\n",
      "- max: 0.89285713\n",
      "- mean: 0.6970681\n",
      "- median: 0.6944444\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.85714287\n",
      "- mean: 0.61619985\n",
      "- median: 0.5294118\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5882353\n",
      "- max: 0.85714287\n",
      "- mean: 0.7120169\n",
      "- median: 0.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def KNN_classifier():\n",
    "    model = KNeighborsClassifier(p=3, n_neighbors=7, metric='cosine')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5\n",
      "iteration: 2 of 5\n",
      "iteration: 3 of 5\n",
      "iteration: 4 of 5\n",
      "iteration: 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['kNN'] = {}\n",
    "accs = accuracies['kNN']\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "\n",
    "# leave out person out validation\n",
    "for ident in range(n):\n",
    "    \n",
    "    print(\"iteration:\", ident+1, \"of\", n)\n",
    "    \n",
    "    train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "    validation_idents = [idents[ident]]\n",
    "    test_idents = [idents[ident-1]]\n",
    "\n",
    "    # Load data\n",
    "    xt_train, y_train = get_data_from_idents(path, train_idents, seconds)\n",
    "    xt_valid, y_valid = get_data_from_idents(path, validation_idents, seconds)\n",
    "    xt_test, y_test = get_data_from_idents(path, test_idents, seconds)\n",
    "\n",
    "    # Scale with standard scaler\n",
    "    sscaler = StandardScaler()\n",
    "    sscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = sscaler.transform(xt_train)\n",
    "    xt_valid = sscaler.transform(xt_valid)\n",
    "    xt_test = sscaler.transform(xt_test)\n",
    "\n",
    "    # Scale with MinMax to range [0,1]\n",
    "    mmscaler = MinMaxScaler()\n",
    "    mmscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = mmscaler.transform(xt_train)\n",
    "    xt_valid = mmscaler.transform(xt_valid)\n",
    "    xt_test = mmscaler.transform(xt_test)\n",
    "\n",
    "    # AE Training params\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "\n",
    "    # undercomplete AE\n",
    "    autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"undercomplete AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = KNN_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"undercomplete CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['undercomplete'].append(curr_acc)\n",
    "    \n",
    "    # sparse AE\n",
    "    autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"sparse AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = KNN_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"sparse CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['sparse'].append(curr_acc)\n",
    "\n",
    "    # deep AE\n",
    "    autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "#     print(\"deep AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = KNN_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"deep CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['deep'].append(curr_acc)\n",
    "    \n",
    "    # contractive AE\n",
    "    autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"contractive AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = KNN_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"contractive CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['contractive'].append(curr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.617647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   undercomplete    sparse      deep  contractive\n",
       "0       0.714286  0.821429  0.714286     0.607143\n",
       "1       0.676471  0.676471  0.676471     0.764706\n",
       "2       0.694444  0.638889  0.611111     0.666667\n",
       "3       0.588235  0.588235  0.382353     0.617647\n",
       "4       0.733333  0.700000  0.566667     0.633333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undercomplete accuracies:\n",
      "- min: 0.5882352941176471\n",
      "- max: 0.7333333333333333\n",
      "- mean: 0.6813538748832866\n",
      "- median: 0.6944444444444444\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5882352941176471\n",
      "- max: 0.8214285714285714\n",
      "- mean: 0.6850046685340804\n",
      "- median: 0.6764705882352942\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.38235294117647056\n",
      "- max: 0.7142857142857143\n",
      "- mean: 0.5901774042950514\n",
      "- median: 0.6111111111111112\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.6071428571428571\n",
      "- max: 0.7647058823529411\n",
      "- mean: 0.6578991596638655\n",
      "- median: 0.6333333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def SVC_classifier():\n",
    "    model = SVC(kernel='rbf', C=1.5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5\n",
      "iteration: 2 of 5\n",
      "iteration: 3 of 5\n",
      "iteration: 4 of 5\n",
      "iteration: 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['SVC'] = {}\n",
    "accs = accuracies['SVC']\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "\n",
    "# leave out person out validation\n",
    "for ident in range(n):\n",
    "    \n",
    "    print(\"iteration:\", ident+1, \"of\", n)\n",
    "    \n",
    "    train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "    validation_idents = [idents[ident]]\n",
    "    test_idents = [idents[ident-1]]\n",
    "\n",
    "    # Load data\n",
    "    xt_train, y_train = get_data_from_idents(path, train_idents, seconds)\n",
    "    xt_valid, y_valid = get_data_from_idents(path, validation_idents, seconds)\n",
    "    xt_test, y_test = get_data_from_idents(path, test_idents, seconds)\n",
    "\n",
    "    # Scale with standard scaler\n",
    "    sscaler = StandardScaler()\n",
    "    sscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = sscaler.transform(xt_train)\n",
    "    xt_valid = sscaler.transform(xt_valid)\n",
    "    xt_test = sscaler.transform(xt_test)\n",
    "\n",
    "    # Scale with MinMax to range [0,1]\n",
    "    mmscaler = MinMaxScaler()\n",
    "    mmscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = mmscaler.transform(xt_train)\n",
    "    xt_valid = mmscaler.transform(xt_valid)\n",
    "    xt_test = mmscaler.transform(xt_test)\n",
    "\n",
    "    # AE Training params\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "\n",
    "    # undercomplete AE\n",
    "    autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"undercomplete AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = SVC_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"undercomplete CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['undercomplete'].append(curr_acc)\n",
    "    \n",
    "    # sparse AE\n",
    "    autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"sparse AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = SVC_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"sparse CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['sparse'].append(curr_acc)\n",
    "\n",
    "    # deep AE\n",
    "    autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "#     print(\"deep AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = SVC_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"deep CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['deep'].append(curr_acc)\n",
    "    \n",
    "    # contractive AE\n",
    "    autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"contractive AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = SVC_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"contractive CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['contractive'].append(curr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.617647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   undercomplete    sparse      deep  contractive\n",
       "0       0.642857  0.678571  0.607143     0.714286\n",
       "1       0.529412  0.529412  0.558824     0.617647\n",
       "2       0.500000  0.694444  0.611111     0.666667\n",
       "3       0.500000  0.500000  0.441176     0.500000\n",
       "4       0.500000  0.733333  0.600000     0.600000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undercomplete accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.6428571428571429\n",
      "- mean: 0.5344537815126051\n",
      "- median: 0.5\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.7333333333333333\n",
      "- mean: 0.6271521942110179\n",
      "- median: 0.6785714285714286\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.4411764705882353\n",
      "- max: 0.6111111111111112\n",
      "- mean: 0.5636507936507937\n",
      "- median: 0.6\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.7142857142857143\n",
      "- mean: 0.6197198879551821\n",
      "- median: 0.6176470588235294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def random_forest_classifier():\n",
    "    model = RandomForestClassifier(n_estimators = 250,\n",
    "                                     min_samples_split = 10,\n",
    "                                     min_samples_leaf = 4,\n",
    "                                     max_features = 'auto',\n",
    "                                     max_depth = 90,\n",
    "                                     bootstrap = True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5\n",
      "iteration: 2 of 5\n",
      "iteration: 3 of 5\n",
      "iteration: 4 of 5\n",
      "iteration: 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['random_forest'] = {}\n",
    "accs = accuracies['random_forest']\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "\n",
    "# leave out person out validation\n",
    "for ident in range(n):\n",
    "    \n",
    "    print(\"iteration:\", ident+1, \"of\", n)\n",
    "    \n",
    "    train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "    validation_idents = [idents[ident]]\n",
    "    test_idents = [idents[ident-1]]\n",
    "\n",
    "    # Load data\n",
    "    xt_train, y_train = get_data_from_idents(path, train_idents, seconds)\n",
    "    xt_valid, y_valid = get_data_from_idents(path, validation_idents, seconds)\n",
    "    xt_test, y_test = get_data_from_idents(path, test_idents, seconds)\n",
    "\n",
    "    # Scale with standard scaler\n",
    "    sscaler = StandardScaler()\n",
    "    sscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = sscaler.transform(xt_train)\n",
    "    xt_valid = sscaler.transform(xt_valid)\n",
    "    xt_test = sscaler.transform(xt_test)\n",
    "\n",
    "    # Scale with MinMax to range [0,1]\n",
    "    mmscaler = MinMaxScaler()\n",
    "    mmscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = mmscaler.transform(xt_train)\n",
    "    xt_valid = mmscaler.transform(xt_valid)\n",
    "    xt_test = mmscaler.transform(xt_test)\n",
    "\n",
    "    # AE Training params\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "\n",
    "    # undercomplete AE\n",
    "    autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"undercomplete AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = random_forest_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"undercomplete CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['undercomplete'].append(curr_acc)\n",
    "    \n",
    "    # sparse AE\n",
    "    autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"sparse AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = random_forest_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"sparse CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['sparse'].append(curr_acc)\n",
    "\n",
    "    # deep AE\n",
    "    autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "#     print(\"deep AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = random_forest_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"deep CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['deep'].append(curr_acc)\n",
    "    \n",
    "    # contractive AE\n",
    "    autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"contractive AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = random_forest_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"contractive CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['contractive'].append(curr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   undercomplete    sparse      deep  contractive\n",
       "0       1.000000  0.964286  0.964286     1.000000\n",
       "1       0.676471  0.794118  0.500000     0.647059\n",
       "2       0.833333  0.722222  0.722222     0.777778\n",
       "3       0.500000  0.558824  0.500000     0.529412\n",
       "4       0.500000  0.566667  0.433333     0.600000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undercomplete accuracies:\n",
      "- min: 0.5\n",
      "- max: 1.0\n",
      "- mean: 0.7019607843137254\n",
      "- median: 0.6764705882352942\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5588235294117647\n",
      "- max: 0.9642857142857143\n",
      "- mean: 0.7212231559290382\n",
      "- median: 0.7222222222222222\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.43333333333333335\n",
      "- max: 0.9642857142857143\n",
      "- mean: 0.6239682539682541\n",
      "- median: 0.5\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5294117647058824\n",
      "- max: 1.0\n",
      "- mean: 0.7108496732026144\n",
      "- median: 0.6470588235294118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "def naive_bayesian_classifier():\n",
    "    model = ComplementNB()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5\n",
      "iteration: 2 of 5\n",
      "iteration: 3 of 5\n",
      "iteration: 4 of 5\n",
      "iteration: 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['naive_bayesian'] = {}\n",
    "accs = accuracies['naive_bayesian']\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "\n",
    "# leave out person out validation\n",
    "for ident in range(n):\n",
    "    \n",
    "    print(\"iteration:\", ident+1, \"of\", n)\n",
    "    \n",
    "    train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "    validation_idents = [idents[ident]]\n",
    "    test_idents = [idents[ident-1]]\n",
    "\n",
    "    # Load data\n",
    "    xt_train, y_train = get_data_from_idents(path, train_idents, seconds)\n",
    "    xt_valid, y_valid = get_data_from_idents(path, validation_idents, seconds)\n",
    "    xt_test, y_test = get_data_from_idents(path, test_idents, seconds)\n",
    "\n",
    "    # Scale with standard scaler\n",
    "    sscaler = StandardScaler()\n",
    "    sscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = sscaler.transform(xt_train)\n",
    "    xt_valid = sscaler.transform(xt_valid)\n",
    "    xt_test = sscaler.transform(xt_test)\n",
    "\n",
    "    # Scale with MinMax to range [0,1]\n",
    "    mmscaler = MinMaxScaler()\n",
    "    mmscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = mmscaler.transform(xt_train)\n",
    "    xt_valid = mmscaler.transform(xt_valid)\n",
    "    xt_test = mmscaler.transform(xt_test)\n",
    "\n",
    "    # AE Training params\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "\n",
    "    # undercomplete AE\n",
    "    autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"undercomplete AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = naive_bayesian_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"undercomplete CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['undercomplete'].append(curr_acc)\n",
    "    \n",
    "    # sparse AE\n",
    "    autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"sparse AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = naive_bayesian_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"sparse CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['sparse'].append(curr_acc)\n",
    "\n",
    "    # deep AE\n",
    "    autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "#     print(\"deep AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = naive_bayesian_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"deep CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['deep'].append(curr_acc)\n",
    "    \n",
    "    # contractive AE\n",
    "    autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"contractive AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = naive_bayesian_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"contractive CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['contractive'].append(curr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   undercomplete    sparse      deep  contractive\n",
       "0       0.571429  0.464286  0.500000     0.535714\n",
       "1       0.529412  0.470588  0.500000     0.500000\n",
       "2       0.472222  0.527778  0.472222     0.472222\n",
       "3       0.500000  0.500000  0.500000     0.500000\n",
       "4       0.500000  0.500000  0.533333     0.500000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undercomplete accuracies:\n",
      "- min: 0.4722222222222222\n",
      "- max: 0.5714285714285714\n",
      "- mean: 0.5146125116713353\n",
      "- median: 0.5\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.4642857142857143\n",
      "- max: 0.5277777777777778\n",
      "- mean: 0.49253034547152197\n",
      "- median: 0.5\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.4722222222222222\n",
      "- max: 0.5333333333333333\n",
      "- mean: 0.5011111111111111\n",
      "- median: 0.5\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.4722222222222222\n",
      "- max: 0.5357142857142857\n",
      "- mean: 0.5015873015873016\n",
      "- median: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGBoost_classifier():\n",
    "    model = XGBClassifier(n_estimators = 83)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5\n",
      "iteration: 2 of 5\n",
      "iteration: 3 of 5\n",
      "iteration: 4 of 5\n",
      "iteration: 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['XGBoost'] = {}\n",
    "accs = accuracies['XGBoost']\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "\n",
    "# leave out person out validation\n",
    "for ident in range(n):\n",
    "    \n",
    "    print(\"iteration:\", ident+1, \"of\", n)\n",
    "    \n",
    "    train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "    validation_idents = [idents[ident]]\n",
    "    test_idents = [idents[ident-1]]\n",
    "\n",
    "    # Load data\n",
    "    xt_train, y_train = get_data_from_idents(path, train_idents, seconds)\n",
    "    xt_valid, y_valid = get_data_from_idents(path, validation_idents, seconds)\n",
    "    xt_test, y_test = get_data_from_idents(path, test_idents, seconds)\n",
    "\n",
    "    # Scale with standard scaler\n",
    "    sscaler = StandardScaler()\n",
    "    sscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = sscaler.transform(xt_train)\n",
    "    xt_valid = sscaler.transform(xt_valid)\n",
    "    xt_test = sscaler.transform(xt_test)\n",
    "\n",
    "    # Scale with MinMax to range [0,1]\n",
    "    mmscaler = MinMaxScaler()\n",
    "    mmscaler.fit(np.vstack((xt_train, xt_test, xt_valid)))\n",
    "    xt_train = mmscaler.transform(xt_train)\n",
    "    xt_valid = mmscaler.transform(xt_valid)\n",
    "    xt_test = mmscaler.transform(xt_test)\n",
    "\n",
    "    # AE Training params\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "\n",
    "    # undercomplete AE\n",
    "    autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"undercomplete AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = XGBoost_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"undercomplete CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['undercomplete'].append(curr_acc)\n",
    "    \n",
    "    # sparse AE\n",
    "    autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"sparse AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = XGBoost_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"sparse CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['sparse'].append(curr_acc)\n",
    "\n",
    "    # deep AE\n",
    "    autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "#     print(\"deep AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = XGBoost_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"deep CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['deep'].append(curr_acc)\n",
    "    \n",
    "    # contractive AE\n",
    "    autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "#     print(\"contractive AUTOENCODER TRAINING: \", ident)\n",
    "    sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                           xt_valid, xt_valid, xt_test, xt_test)\n",
    "    model = XGBoost_classifier()\n",
    "    xtt_train = encoded.predict(xt_train)\n",
    "    xtt_test = encoded.predict(xt_test)\n",
    "#     print(\"contractive CLASSIFICATION TRAINING: \", ident)\n",
    "    model.fit(xtt_train, y_train)\n",
    "    curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "    accs['contractive'].append(curr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.676471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   undercomplete    sparse      deep  contractive\n",
       "0       1.000000  1.000000  1.000000     1.000000\n",
       "1       0.676471  0.676471  0.588235     0.676471\n",
       "2       0.888889  0.722222  0.722222     0.694444\n",
       "3       0.647059  0.735294  0.558824     0.647059\n",
       "4       0.766667  0.566667  0.666667     0.533333"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undercomplete accuracies:\n",
      "- min: 0.6470588235294118\n",
      "- max: 1.0\n",
      "- mean: 0.7958169934640522\n",
      "- median: 0.7666666666666667\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5666666666666667\n",
      "- max: 1.0\n",
      "- mean: 0.7401307189542484\n",
      "- median: 0.7222222222222222\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5588235294117647\n",
      "- max: 1.0\n",
      "- mean: 0.7071895424836602\n",
      "- median: 0.6666666666666666\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5333333333333333\n",
      "- max: 1.0\n",
      "- mean: 0.7102614379084967\n",
      "- median: 0.6764705882352942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Compare Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print min, max, mean, median for each clasifier/autoencoder combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_dense:\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.6470588235294118\n",
      "   - max: 1.0\n",
      "   - mean: 0.7958169934640522\n",
      "   - median: 0.7666666666666667\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7401307189542484\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5588235294117647\n",
      "   - max: 1.0\n",
      "   - mean: 0.7071895424836602\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5333333333333333\n",
      "   - max: 1.0\n",
      "   - mean: 0.7102614379084967\n",
      "   - median: 0.6764705882352942\n",
      "\n",
      "\n",
      "\n",
      "LSTM:\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.6470588235294118\n",
      "   - max: 1.0\n",
      "   - mean: 0.7958169934640522\n",
      "   - median: 0.7666666666666667\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7401307189542484\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5588235294117647\n",
      "   - max: 1.0\n",
      "   - mean: 0.7071895424836602\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5333333333333333\n",
      "   - max: 1.0\n",
      "   - mean: 0.7102614379084967\n",
      "   - median: 0.6764705882352942\n",
      "\n",
      "\n",
      "\n",
      "kNN:\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.6470588235294118\n",
      "   - max: 1.0\n",
      "   - mean: 0.7958169934640522\n",
      "   - median: 0.7666666666666667\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7401307189542484\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5588235294117647\n",
      "   - max: 1.0\n",
      "   - mean: 0.7071895424836602\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5333333333333333\n",
      "   - max: 1.0\n",
      "   - mean: 0.7102614379084967\n",
      "   - median: 0.6764705882352942\n",
      "\n",
      "\n",
      "\n",
      "SVC:\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.6470588235294118\n",
      "   - max: 1.0\n",
      "   - mean: 0.7958169934640522\n",
      "   - median: 0.7666666666666667\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7401307189542484\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5588235294117647\n",
      "   - max: 1.0\n",
      "   - mean: 0.7071895424836602\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5333333333333333\n",
      "   - max: 1.0\n",
      "   - mean: 0.7102614379084967\n",
      "   - median: 0.6764705882352942\n",
      "\n",
      "\n",
      "\n",
      "random_forest:\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.6470588235294118\n",
      "   - max: 1.0\n",
      "   - mean: 0.7958169934640522\n",
      "   - median: 0.7666666666666667\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7401307189542484\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5588235294117647\n",
      "   - max: 1.0\n",
      "   - mean: 0.7071895424836602\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5333333333333333\n",
      "   - max: 1.0\n",
      "   - mean: 0.7102614379084967\n",
      "   - median: 0.6764705882352942\n",
      "\n",
      "\n",
      "\n",
      "naive_bayesian:\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.6470588235294118\n",
      "   - max: 1.0\n",
      "   - mean: 0.7958169934640522\n",
      "   - median: 0.7666666666666667\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7401307189542484\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5588235294117647\n",
      "   - max: 1.0\n",
      "   - mean: 0.7071895424836602\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5333333333333333\n",
      "   - max: 1.0\n",
      "   - mean: 0.7102614379084967\n",
      "   - median: 0.6764705882352942\n",
      "\n",
      "\n",
      "\n",
      "XGBoost:\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.6470588235294118\n",
      "   - max: 1.0\n",
      "   - mean: 0.7958169934640522\n",
      "   - median: 0.7666666666666667\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7401307189542484\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5588235294117647\n",
      "   - max: 1.0\n",
      "   - mean: 0.7071895424836602\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5333333333333333\n",
      "   - max: 1.0\n",
      "   - mean: 0.7102614379084967\n",
      "   - median: 0.6764705882352942\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in accuracies:\n",
    "    print(classifier + \":\")\n",
    "    for key in accuracies[classifier]:\n",
    "        print(\"  \" + key, \"accuracies:\")\n",
    "        print(\"   - min:\", np.min(accs[key]))\n",
    "        print(\"   - max:\", np.max(accs[key]))\n",
    "        print(\"   - mean:\", np.mean(accs[key]))\n",
    "        print(\"   - median:\", np.median(accs[key]))\n",
    "        print(\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all accuracies in table form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_dense:\n",
      "   undercomplete    sparse      deep  contractive\n",
      "0       0.821429  0.821429  0.785714     0.785714\n",
      "1       0.735294  0.735294  0.676471     0.588235\n",
      "2       0.638889  0.638889  0.638889     0.472222\n",
      "3       0.558824  0.588235  0.588235     0.647059\n",
      "4       0.566667  0.600000  0.500000     0.566667\n",
      "\n",
      "\n",
      "LSTM:\n",
      "   undercomplete    sparse      deep  contractive\n",
      "0       0.785714  0.892857  0.857143     0.857143\n",
      "1       0.558824  0.735294  0.529412     0.764706\n",
      "2       0.694444  0.694444  0.694444     0.750000\n",
      "3       0.588235  0.529412  0.500000     0.588235\n",
      "4       0.666667  0.633333  0.500000     0.600000\n",
      "\n",
      "\n",
      "kNN:\n",
      "   undercomplete    sparse      deep  contractive\n",
      "0       0.714286  0.821429  0.714286     0.607143\n",
      "1       0.676471  0.676471  0.676471     0.764706\n",
      "2       0.694444  0.638889  0.611111     0.666667\n",
      "3       0.588235  0.588235  0.382353     0.617647\n",
      "4       0.733333  0.700000  0.566667     0.633333\n",
      "\n",
      "\n",
      "SVC:\n",
      "   undercomplete    sparse      deep  contractive\n",
      "0       0.642857  0.678571  0.607143     0.714286\n",
      "1       0.529412  0.529412  0.558824     0.617647\n",
      "2       0.500000  0.694444  0.611111     0.666667\n",
      "3       0.500000  0.500000  0.441176     0.500000\n",
      "4       0.500000  0.733333  0.600000     0.600000\n",
      "\n",
      "\n",
      "random_forest:\n",
      "   undercomplete    sparse      deep  contractive\n",
      "0       1.000000  0.964286  0.964286     1.000000\n",
      "1       0.676471  0.794118  0.500000     0.647059\n",
      "2       0.833333  0.722222  0.722222     0.777778\n",
      "3       0.500000  0.558824  0.500000     0.529412\n",
      "4       0.500000  0.566667  0.433333     0.600000\n",
      "\n",
      "\n",
      "naive_bayesian:\n",
      "   undercomplete    sparse      deep  contractive\n",
      "0       0.571429  0.464286  0.500000     0.535714\n",
      "1       0.529412  0.470588  0.500000     0.500000\n",
      "2       0.472222  0.527778  0.472222     0.472222\n",
      "3       0.500000  0.500000  0.500000     0.500000\n",
      "4       0.500000  0.500000  0.533333     0.500000\n",
      "\n",
      "\n",
      "XGBoost:\n",
      "   undercomplete    sparse      deep  contractive\n",
      "0       1.000000  1.000000  1.000000     1.000000\n",
      "1       0.676471  0.676471  0.588235     0.676471\n",
      "2       0.888889  0.722222  0.722222     0.694444\n",
      "3       0.647059  0.735294  0.558824     0.647059\n",
      "4       0.766667  0.566667  0.666667     0.533333\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in accuracies:\n",
    "    print(classifier + \":\")\n",
    "    print(pandas.DataFrame.from_dict(accuracies[classifier]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37talos",
   "language": "python",
   "name": "py37talos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
