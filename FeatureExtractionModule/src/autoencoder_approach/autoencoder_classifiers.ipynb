{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "Exploring different classifiers with different autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoders:  \n",
    "[Undercomplete Autoencoder](#Undercomplete-Autoencoder)  \n",
    "[Sparse Autoencoder](#Sparse-Autoencoder)  \n",
    "[Deep Autoencoder](#Deep-Autoencoder)  \n",
    "[Contractive Autoencoder](#Contractive-Autoencoder)  \n",
    "\n",
    "classifiers:  \n",
    "[Simple dense layer](#Simple-dense-layer)  \n",
    "[LSTM-based classifier](#LSTM-based-classifier)  \n",
    "[kNN](#kNN)  \n",
    "[SVC](#SVC)  \n",
    "[Random Forest](#Random-Forest)  \n",
    "[XGBoost](#XGBoost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datareader # made by the previous author for reading the collected data\n",
    "import dataextractor # same as above\n",
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# need to disable eager execution for .get_weights() in contractive autoencoder loss to work\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "# required for the contractive autoencoder\n",
    "import tensorflow.keras.backend as K\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import warnings\n",
    "\n",
    "import talos\n",
    "from talos.utils import lr_normalizer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.keras.backend.set_floatx('float32') # call this, to set keras to use float32 to avoid a warning message\n",
    "metrics = ['accuracy']#,\n",
    "#            keras.metrics.TruePositives(),\n",
    "#            keras.metrics.FalsePositives(),\n",
    "#            keras.metrics.TrueNegatives(),\n",
    "#            keras.metrics.FalseNegatives()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/ageron/handson-ml/blob/master/extra_tensorflow_reproducibility.ipynb\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                        inter_op_parallelism_threads=1)\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    #... this will run single threaded\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(4)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the notebook in the terminal with \"PYTHONHASHSEED=0 jupyter notebook\" \n",
    "# or in anaconda \"set PYTHONHASHSEED=0\" then start jupyter notebook\n",
    "import os\n",
    "if os.environ.get(\"PYTHONHASHSEED\") != \"0\":\n",
    "    raise Exception(\"You must set PYTHONHASHSEED=0 when starting the Jupyter server to get reproducible results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original author's code, just copied into separate cells of this jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_busy_vs_relax_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data from either 'on task' or 'relax' time frames and their class (0 or 1).\n",
    "    TODO: join functions\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### task versus relax (1 sample each)\n",
    "        dataextract = dataextractor.DataExtractor(data[0][busy_n[task_num_table][0]:busy_n[task_num_table][1]],\n",
    "                                                  data[1][busy_n[task_num_table][0]:busy_n[task_num_table][1]],\n",
    "                                                  samp_rate)\n",
    "\n",
    "        dataextract_relax = dataextractor.DataExtractor(data[0][relax_n[task_num_table][0]:relax_n[task_num_table][1]],\n",
    "                                                        data[1][relax_n[task_num_table][0]:relax_n[task_num_table][1]],\n",
    "                                                        samp_rate)\n",
    "        try:\n",
    "            tasks_data = np.vstack((tasks_data, dataextract.y[-samp_rate * seconds:]))\n",
    "            tasks_y = np.vstack((tasks_y, 1))\n",
    "            tasks_data = np.vstack((tasks_data, dataextract_relax.y[-samp_rate * seconds:]))\n",
    "            tasks_y = np.vstack((tasks_y, 0))\n",
    "        except ValueError:\n",
    "            continue\n",
    "#             print(ident)  # ignore short windows\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engagement_increase_vs_decrease_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data from either engagement 'increase' or 'decrease' time frames and their class (0 or 1).\n",
    "    TODO: join functions\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds * samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### engagement increase / decrease\n",
    "        if task_num_table == 0:\n",
    "            continue\n",
    "        mid = int((relax_n[task_num_table][0] + relax_n[task_num_table][1])/2)\n",
    "        length = int(samp_rate*30)\n",
    "        for j in range(10):\n",
    "            new_end = int(mid-j*samp_rate)\n",
    "\n",
    "            new_start2 = int(mid+j*samp_rate)\n",
    "\n",
    "            dataextract_decrease = dataextractor.DataExtractor(data[0][new_end - length:new_end],\n",
    "                                                               data[1][new_end-length:new_end],\n",
    "                                                               samp_rate)\n",
    "\n",
    "            dataextract_increase = dataextractor.DataExtractor(data[0][new_start2: new_start2 + length],\n",
    "                                                               data[1][new_start2: new_start2 + length], samp_rate)\n",
    "\n",
    "            try:\n",
    "                tasks_data = np.vstack((tasks_data, dataextract_increase.y))\n",
    "                tasks_y = np.vstack((tasks_y, 1))\n",
    "                tasks_data = np.vstack((tasks_data, dataextract_decrease.y))\n",
    "                tasks_y = np.vstack((tasks_y, 0))\n",
    "            except ValueError:\n",
    "                print(ident)  # ignore short windows\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_complexities_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data along with task complexity class.\n",
    "    TODO: join functions. Add parameter to choose different task types and complexities\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### task complexity classification\n",
    "        if cog_res['task_complexity'][task_num_table] == 'medium':\n",
    "            continue\n",
    "        # if cog_res['task_label'][task_num_table] == 'FA' or cog_res['task_label'][task_num_table] == 'HP':\n",
    "        #     continue\n",
    "        if cog_res['task_label'][task_num_table] != 'NC':\n",
    "            continue\n",
    "        map_compl = {\n",
    "            'low': 0,\n",
    "            'medium': 2,\n",
    "            'high': 1\n",
    "        }\n",
    "        for j in range(10):\n",
    "            new_end = int(busy_n[task_num_table][1] - j * samp_rate)\n",
    "            new_start = int(new_end - samp_rate*30)\n",
    "            dataextract = dataextractor.DataExtractor(data[0][new_start:new_end],\n",
    "                                                      data[1][new_start:new_end], samp_rate)\n",
    "            try:\n",
    "                tasks_data = np.vstack((tasks_data, dataextract.y))\n",
    "                tasks_y = np.vstack((tasks_y, map_compl.get(cog_res['task_complexity'][task_num_table])))\n",
    "            except ValueError:\n",
    "                print(ident)\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TLX_timeframes(path, ident, seconds):\n",
    "    \"\"\"Returns raw data along with task load index class.\n",
    "    TODO: join functions. Add parameter to choose different task types and complexities\"\"\"\n",
    "\n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "\n",
    "        ### task load index\n",
    "        if cog_res['task_complexity'][task_num_table] == 'medium' or cog_res['task_label'][task_num_table] != 'PT':\n",
    "            continue\n",
    "        for j in range(10):\n",
    "            new_end = int(busy_n[task_num_table][1] - j * samp_rate)\n",
    "            new_start = int(new_end - samp_rate*30)\n",
    "            dataextract = dataextractor.DataExtractor(data[0][new_start:new_end],\n",
    "                                                      data[1][new_start:new_end], samp_rate)\n",
    "            try:\n",
    "                tasks_data = np.vstack((tasks_data, dataextract.y))\n",
    "                tasks_y = np.vstack((tasks_y, cog_res['task_load_index'][task_num_table]))\n",
    "            except ValueError:\n",
    "                print(ident)\n",
    "\n",
    "    return tasks_data, tasks_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_idents(path, idents, seconds):\n",
    "    \"\"\"Go through all user data and take out windows of only <seconds> long time frames,\n",
    "    along with the given class (from 'divide_each_task' function).\n",
    "    \"\"\"\n",
    "    samp_rate = 43  # hard-coded sample rate\n",
    "    data, ys = np.empty((0, samp_rate*seconds)), np.empty((0, 1))\n",
    "    for i in idents:\n",
    "        x, y = get_busy_vs_relax_timeframes(path, i, seconds) # either 'get_busy_vs_relax_timeframes',\n",
    "        # get_engagement_increase_vs_decrease_timeframes, get_task_complexities_timeframes or get_TLX_timeframes\n",
    "        # TODO: ^ modify, so that different functions can be accessible by parameter\n",
    "        data = np.vstack((data, x))\n",
    "        ys = np.vstack((ys, y))\n",
    "    return data, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, x_train, y_train, batch_size, epochs, x_valid, y_valid, x_test, y_test):\n",
    "    \"\"\"Train model with the given training, validation, and test set, with appropriate batch size and # epochs.\"\"\"\n",
    "    epoch_data = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_valid, y_valid), verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    acc = score[1]\n",
    "    score = score[0]\n",
    "    return score, acc, epoch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(x, maxlen):\n",
    "    \"\"\"Pad sequences (all have to be same length).\"\"\"\n",
    "    print('Pad sequences (samples x time)')\n",
    "    return sequence.pad_sequences(x, maxlen=maxlen, dtype=np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_busy_vs_relax_timeframes_br_hb(path, ident, seconds):\n",
    "    \"\"\"Returns raw data from either 'on task' or 'relax' time frames and their class (0 or 1).\"\"\"\n",
    "    \n",
    "    dataread = datareader.DataReader(path, ident)  # initialize path to data\n",
    "    data = dataread.read_grc_data()  # read from files\n",
    "    samp_rate = int(round(len(data[1]) / max(data[0])))\n",
    "    cog_res = dataread.read_cognitive_load_study(str(ident) + '-primary-extract.txt')\n",
    "\n",
    "    tasks_data = np.empty((0, seconds*samp_rate))\n",
    "    tasks_y = np.empty((0, 1))\n",
    "    breathing = np.empty((0,12))\n",
    "    heartbeat = np.empty((0,10))\n",
    "\n",
    "    busy_n = dataread.get_data_task_timestamps(return_indexes=True)\n",
    "    relax_n = dataread.get_relax_timestamps(return_indexes=True)\n",
    "\n",
    "    for i in cog_res['task_number']:\n",
    "        task_num_table = i - 225  # 0 - 17\n",
    "        tmp_tasks_data = np.empty((0, seconds*samp_rate))\n",
    "        tmp_tasks_y = np.empty((0, 1))\n",
    "        tmp_breathing = np.empty((0,12))\n",
    "        tmp_heartbeat = np.empty((0,10))\n",
    "\n",
    "        ### task versus relax (1 sample each)\n",
    "        dataextract = dataextractor.DataExtractor(data[0][busy_n[task_num_table][0]:busy_n[task_num_table][1]],\n",
    "                                                  data[1][busy_n[task_num_table][0]:busy_n[task_num_table][1]],\n",
    "                                                  samp_rate)\n",
    "\n",
    "        dataextract_relax = dataextractor.DataExtractor(data[0][relax_n[task_num_table][0]:relax_n[task_num_table][1]],\n",
    "                                                        data[1][relax_n[task_num_table][0]:relax_n[task_num_table][1]],\n",
    "                                                        samp_rate)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # get extracted features for breathing\n",
    "            tmpBR_busy = dataextract.extract_from_breathing_time(dataextract.t[-samp_rate*seconds:],\n",
    "                                                                 dataextract.y[-samp_rate*seconds:])\n",
    "            tmpBR_relax = dataextract_relax.extract_from_breathing_time(dataextract_relax.t[-samp_rate*seconds:],\n",
    "                                                                 dataextract_relax.y[-samp_rate*seconds:])\n",
    "            #get extracted features for heartbeat\n",
    "            tmpHB_busy = dataextract.extract_from_heartbeat_time(dataextract.t[-samp_rate*seconds:],\n",
    "                                                                 dataextract.y[-samp_rate*seconds:])\n",
    "            tmpHB_relax = dataextract.extract_from_heartbeat_time(dataextract_relax.t[-samp_rate*seconds:],\n",
    "                                                                 dataextract_relax.y[-samp_rate*seconds:])\n",
    "\n",
    "            tmp_tasks_data = np.vstack((tmp_tasks_data, dataextract.y[-samp_rate * seconds:]))\n",
    "            tmp_tasks_y = np.vstack((tasks_y, 1))\n",
    "            tmp_tasks_data = np.vstack((tmp_tasks_data, dataextract_relax.y[-samp_rate * seconds:]))\n",
    "            tmp_tasks_y = np.vstack((tmp_tasks_y, 0))\n",
    "\n",
    "            # put busy frames then relaxed frames under the previous frames\n",
    "            tmp_breathing = np.vstack((tmp_breathing, tmpBR_busy.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "            tmp_breathing = np.vstack((tmp_breathing, tmpBR_relax.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "\n",
    "            tmp_heartbeat = np.vstack((tmp_heartbeat, tmpHB_busy.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "            tmp_heartbeat = np.vstack((tmp_heartbeat, tmpHB_relax.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "\n",
    "        except ValueError:\n",
    "#             print(ident)  # ignore short windows\n",
    "            continue\n",
    "\n",
    "        # put busy frames then relaxed frames under the previous frames\n",
    "        tasks_data = np.vstack((tasks_data, dataextract.y[-samp_rate * seconds:]))\n",
    "        tasks_y = np.vstack((tasks_y, 1))\n",
    "        tasks_data = np.vstack((tasks_data, dataextract_relax.y[-samp_rate * seconds:]))\n",
    "        tasks_y = np.vstack((tasks_y, 0))\n",
    "\n",
    "        breathing = np.vstack((breathing, tmpBR_busy.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "        breathing = np.vstack((breathing, tmpBR_relax.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "\n",
    "        heartbeat = np.vstack((heartbeat, tmpHB_busy.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "        heartbeat = np.vstack((heartbeat, tmpHB_relax.to_numpy(dtype='float64', na_value=0)[0][:-1]))\n",
    "\n",
    "    return tasks_data, tasks_y, breathing, heartbeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_idents_br_hb(path, idents, seconds):\n",
    "    \"\"\"Go through all user data and take out windows of only <seconds> long time frames,\n",
    "    along with the given class (from 'divide_each_task' function).\n",
    "    \"\"\"\n",
    "    samp_rate = 43  # hard-coded sample rate\n",
    "    data, ys = np.empty((0, samp_rate*seconds)), np.empty((0, 1))\n",
    "    brs = np.empty((0,12))\n",
    "    hbs = np.empty((0,10))\n",
    "    combined = np.empty((0,22))\n",
    "    \n",
    "    # was gettign some weird warnings; stack overflow said to ignore them\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        for i in idents:\n",
    "            x, y, br, hb = get_busy_vs_relax_timeframes_br_hb(path, i, seconds) # either 'get_busy_vs_relax_timeframes',\n",
    "            # get_engagement_increase_vs_decrease_timeframes, get_task_complexities_timeframes or get_TLX_timeframes\n",
    "\n",
    "            data = np.vstack((data, x))\n",
    "            ys = np.vstack((ys, y))\n",
    "            brs = np.vstack((brs, br))\n",
    "            hbs = np.vstack((hbs, hb))\n",
    "        combined = np.hstack((brs,hbs))\n",
    "    \n",
    "    return data, ys, brs, hbs, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(x_train, x_valid, x_test, standardScaler=True, minMaxScaler=True):\n",
    "    \n",
    "    # copy data variables\n",
    "    xt_train = x_train\n",
    "    xt_valid = x_valid\n",
    "    xt_test = x_test\n",
    "    \n",
    "    if standardScaler:\n",
    "        # Scale with standard scaler\n",
    "        sscaler = StandardScaler()\n",
    "        sscaler.fit(np.vstack((xt_train, xt_valid, xt_test)))\n",
    "        xt_train = sscaler.transform(xt_train)\n",
    "        xt_valid = sscaler.transform(xt_valid)\n",
    "        xt_test = sscaler.transform(xt_test)\n",
    "\n",
    "    if minMaxScaler:\n",
    "        # Scale with MinMax to range [0,1]\n",
    "        mmscaler = MinMaxScaler((0,1))\n",
    "        mmscaler.fit(np.vstack((xt_train, xt_valid, xt_test)))\n",
    "        xt_train = mmscaler.transform(xt_train)\n",
    "        xt_valid = mmscaler.transform(xt_valid)\n",
    "        xt_test = mmscaler.transform(xt_test)\n",
    "    \n",
    "    return xt_train, xt_valid, xt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accs is a dictionary which holds 1d arrays of accuracies in each key\n",
    "# except the key 'test id' which holds strings of the id which yielded the coresponding accuracies\n",
    "def print_accs_stats(accs):\n",
    "    # loop over each key\n",
    "    for key in accs:\n",
    "    \n",
    "        if (key == 'test id'):\n",
    "            # skip calculating ids\n",
    "            continue\n",
    "\n",
    "        # calculate and print some statistics\n",
    "        print(key, \"accuracies:\")\n",
    "        print(\"- min:\", np.min(accs[key]))\n",
    "        print(\"- max:\", np.max(accs[key]))\n",
    "        print(\"- mean:\", np.mean(accs[key]))\n",
    "        print(\"- median:\", np.median(accs[key]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undercomplete Autoencoder  \n",
    "from https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undercomplete_ae(x, encoding_dim=64, encoded_as_model=False):\n",
    "    # Simplest possible autoencoder from https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_data = Input(shape=x[0].shape, name=\"input\")\n",
    "    dropout = Dropout(0.25, name=\"dropout\")(input_data)\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation='relu', name=\"encoded\")(dropout)\n",
    "    \n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(x[0].shape[0], activation='sigmoid', name=\"decoded\")(encoded)\n",
    "\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "    \n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Autoencoder  \n",
    "from https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_ae(x, encoding_dim=64, encoded_as_model=False):\n",
    "    # Simplest possible autoencoder from https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_data = Input(shape=x[0].shape, name=\"input\")\n",
    "    dropout = Dropout(0.25, name=\"dropout\") (input_data)\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    # add a sparsity constraint\n",
    "    encoded = Dense(encoding_dim, activation='relu', name=\"encoded\",\n",
    "                    activity_regularizer=regularizers.l1(10e-5))(dropout)\n",
    "    \n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(x[0].shape[0], activation='sigmoid', name=\"decoded\")(encoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_data, decoded, name=\"sparse_ae\")\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "    \n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Autoencoder  \n",
    "from https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_ae(x, enc_layers=[512,256], encoding_dim=64, dec_layers=[256,512], encoded_as_model=False):\n",
    "    # From https://www.tensorflow.org/guide/keras/functional#use_the_same_graph_of_layers_to_define_multiple_models\n",
    "    input_data = keras.Input(shape=x[0].shape, name=\"normalized_signal\")\n",
    "    model = Dropout(0.25, name=\"dropout\", autocast=False)(input_data)\n",
    "    for i in enumerate(enc_layers):\n",
    "        model = Dense(i[1], activation=\"relu\", name=\"dense_enc_\" + str(i[0]+1))(model)\n",
    "    encoded_output = Dense(encoding_dim, activation=\"relu\", name=\"encoded_signal\")(model)\n",
    "\n",
    "    encoded = encoded_output\n",
    "\n",
    "    model = layers.Dense(dec_layers[0], activation=\"sigmoid\", name=\"dense_dec_1\")(encoded_output)\n",
    "    for i in enumerate(dec_layers[1:]):\n",
    "        model = Dense(i[1], activation=\"sigmoid\", name=\"dense_dec_\" + str(i[0]+2))(model)\n",
    "    decoded_output = Dense(x[0].shape[0], activation=\"sigmoid\", name=\"reconstructed_signal\")(model)\n",
    "    \n",
    "    autoencoder = Model(input_data, decoded_output, name=\"autoencoder\")\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "\n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contractive Autoencoder\n",
    "From: https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to be able to access the autoencoder in the loss funciton\n",
    "def loss_with_params(autoencoder):\n",
    "    # loss function from https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/\n",
    "    def contractive_loss(y_pred, y_true):\n",
    "\n",
    "        lam = 1e-4\n",
    "        mse = K.mean(K.square(y_true - y_pred), axis=1)\n",
    "\n",
    "        W = K.variable(value=autoencoder.get_layer('encoded').get_weights()[0])  # N x N_hidden\n",
    "        W = K.transpose(W)  # N_hidden x N\n",
    "        h = autoencoder.get_layer('encoded').output\n",
    "        dh = h * (1 - h)  # N_batch x N_hidden\n",
    "\n",
    "        # N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n",
    "        contractive = lam * K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n",
    "\n",
    "        return mse + contractive\n",
    "    return contractive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractive_ae(x, encoding_dim=64, encoded_as_model=False):\n",
    "    # From https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/\n",
    "\n",
    "    input_data = Input(shape=x[0].shape, name=\"input\")\n",
    "    encoded = Dense(encoding_dim, activation='sigmoid', name='encoded')(input_data)\n",
    "    outputs = Dense(x[0].shape[0], activation='linear', name=\"output\")(encoded)\n",
    "\n",
    "    autoencoder = Model(input_data, outputs, name=\"autoencoder\")\n",
    "    \n",
    "    # compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss=loss_with_params(autoencoder), metrics=metrics)\n",
    "    \n",
    "    # if return encoder in the encoded variable\n",
    "    if encoded_as_model:\n",
    "        encoded = Model(input_data, encoded)\n",
    "    \n",
    "    return autoencoder, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary to store accuracies for comparison\n",
    "accuracies = {}\n",
    "\n",
    "# used for reading the data into an array\n",
    "seconds = 30  # time window length\n",
    "idents = ['2gu87', 'iz2ps', '1mpau', '7dwjy', '7swyk', '94mnx', 'bd47a', 'c24ur', 'ctsax', 'dkhty', 'e4gay',\n",
    "              'ef5rq', 'f1gjp', 'hpbxa', 'pmyfl', 'r89k1', 'tn4vl', 'td5pr', 'gyqu9', 'fzchw', 'l53hg', '3n2f9',\n",
    "              '62i9y']\n",
    "path = '../../../StudyData/'\n",
    "\n",
    "# change to len(idents) at the end to use all the data\n",
    "n = 5 #len(idents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_classifier(model, params):\n",
    "    \n",
    "    model = Dropout(params['dropout'], name='dropout_cl')(model)\n",
    "    model = Dense(params['hidden_size'], activation=params['activation'], name='dense_cl1')(model)\n",
    "    model = Dense(1, activation=params['last_activation'], name='dense_cl2')(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_classifier_base():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dropout': 0.24,\n",
    "    'optimizer': 'Adam',\n",
    "    'hidden_size': 64,\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'last_activation': 'sigmoid',\n",
    "    'activation': 'softmax',\n",
    "    'batch_size': 256,\n",
    "    'epochs': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5 ; time elapsed: 0:00:00.005985\n",
      "WARNING:tensorflow:From D:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "iteration: 2 of 5 ; time elapsed: 0:01:02.442774\n",
      "iteration: 3 of 5 ; time elapsed: 0:02:11.879793\n",
      "iteration: 4 of 5 ; time elapsed: 0:03:30.950485\n",
      "iteration: 5 of 5 ; time elapsed: 0:04:56.480864\n",
      "Completed! Time elapsed: 0:06:33.052612\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['simple_dense'] = {}\n",
    "accs = accuracies['simple_dense']\n",
    "accs['phase'] = []\n",
    "accs['breathing'] = []\n",
    "accs['heartbeat'] = []\n",
    "accs['combined br hb'] = []\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "accs['test id'] = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # leave out person out validation\n",
    "    for ident in range(n):\n",
    "\n",
    "        # print current iteration and time elapsed from start\n",
    "        print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "        ## ----- Data preparation:\n",
    "        # Split the data\n",
    "        train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "        validation_idents = [idents[ident]]\n",
    "        test_idents = [idents[ident-1]]\n",
    "        \n",
    "        # save test id to see which id yielded which accuracies\n",
    "        accs['test id'].append(test_idents[0])\n",
    "\n",
    "        # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "        xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hr(path, train_idents, seconds)\n",
    "        xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hr(path, validation_idents, seconds)\n",
    "        xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hr(path, test_idents, seconds)\n",
    "\n",
    "        # Scale data with standard scaler then MinMax scaler\n",
    "        # Raw Phase data:\n",
    "        xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted breathing data:\n",
    "        br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted Heartbeat data:\n",
    "        hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "        cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify without autoencoders:\n",
    "        # Phase classifier:\n",
    "        model = dense_classifier_base()\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['phase'].append(curr_acc)\n",
    "\n",
    "        # Breathing classifier:\n",
    "        model = dense_classifier_base()\n",
    "        sc, curr_acc, epoch_data = model_train(model, br_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               br_valid, y_valid, br_test, y_test)\n",
    "        accs['breathing'].append(curr_acc)\n",
    "\n",
    "        # Heartbeat classifier:\n",
    "        model = dense_classifier_base()\n",
    "        sc, curr_acc, epoch_data = model_train(model, hb_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               hb_valid, y_valid, hb_test, y_test)\n",
    "        accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "        # Combined classifier:\n",
    "        model = dense_classifier_base()\n",
    "        sc, curr_acc, epoch_data = model_train(model, cmb_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               cmb_valid, y_valid, cmb_test, y_test)\n",
    "        accs['combined br hb'].append(curr_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify with autoencoders:\n",
    "        # AE Training params\n",
    "        batch_size = 256\n",
    "        epochs = 100\n",
    "\n",
    "        # Undercomplete AE:\n",
    "        autoencoder, encoded = undercomplete_ae(xt_train, 60)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = dense_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "        # Sparse AE:\n",
    "        autoencoder, encoded = sparse_ae(xt_train, 60)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = dense_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['sparse'].append(curr_acc)\n",
    "\n",
    "        # Deep AE:\n",
    "        autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512])\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = dense_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['deep'].append(curr_acc)\n",
    "\n",
    "        # Contractive AE:\n",
    "        autoencoder, encoded = contractive_ae(xt_train, 60)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = dense_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['contractive'].append(curr_acc)\n",
    "\n",
    "# Print total time required to run this\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed!\", \"Time elapsed:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>breathing</th>\n",
       "      <th>heartbeat</th>\n",
       "      <th>combined br hb</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "      <th>test id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>62i9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>2gu87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>iz2ps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1mpau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7dwjy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
       "0  0.791667   0.416667   0.666667        0.458333       0.750000  0.833333   \n",
       "1  0.647059   0.676471   0.235294        0.617647       0.735294  0.794118   \n",
       "2  0.666667   0.861111   0.361111        0.888889       0.694444  0.638889   \n",
       "3  0.500000   0.558824   0.352941        0.617647       0.529412  0.558824   \n",
       "4  0.566667   0.566667   0.400000        0.566667       0.733333  0.633333   \n",
       "\n",
       "       deep  contractive test id  \n",
       "0  0.708333     0.875000   62i9y  \n",
       "1  0.676471     0.558824   2gu87  \n",
       "2  0.666667     0.472222   iz2ps  \n",
       "3  0.529412     0.500000   1mpau  \n",
       "4  0.466667     0.666667   7dwjy  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.7916667\n",
      "- mean: 0.63441175\n",
      "- median: 0.64705884\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.41666666\n",
      "- max: 0.8611111\n",
      "- mean: 0.6159477\n",
      "- median: 0.56666666\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.23529412\n",
      "- max: 0.6666667\n",
      "- mean: 0.40320262\n",
      "- median: 0.3611111\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.45833334\n",
      "- max: 0.8888889\n",
      "- mean: 0.62983656\n",
      "- median: 0.61764705\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.5294118\n",
      "- max: 0.75\n",
      "- mean: 0.6884967\n",
      "- median: 0.73333335\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5588235\n",
      "- max: 0.8333333\n",
      "- mean: 0.6916993\n",
      "- median: 0.6388889\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.46666667\n",
      "- max: 0.7083333\n",
      "- mean: 0.6095098\n",
      "- median: 0.6666667\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.4722222\n",
      "- max: 0.875\n",
      "- mean: 0.61454254\n",
      "- median: 0.5588235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM-based classifier  \n",
    "based on the original author's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize hyperparameters with talos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_classifier(model, params):\n",
    "\n",
    "    model = layers.Reshape((-1, 1), input_shape=(model.shape), name='reshape_cl') (model)\n",
    "\n",
    "    model = layers.Dropout(params['dropout'], name='dropout_cl1') (model)\n",
    "    \n",
    "    model = Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides'],\n",
    "                     name='conv1d_cl1') (model)\n",
    "    \n",
    "    model = MaxPooling1D(pool_size=params['pool_size'], name='maxpool_cl1') (model)\n",
    "    \n",
    "    model = Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides'],\n",
    "                     name='conv1d_cl2') (model)\n",
    "    \n",
    "    model = MaxPooling1D(pool_size=params['pool_size'], name='maxpool_cl2') (model)\n",
    "    \n",
    "    model = layers.Dropout(params['dropout'], name='dropout_cl2') (model)\n",
    "\n",
    "    model = LSTM(params['lstm_output_size'], activation='sigmoid', name='lstm_cl') (model)\n",
    "\n",
    "    model = Dense(1, activation=params['last_activation'], name='dense_cl') (model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_classifier_base(params):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides']))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=params['pool_size']))\n",
    "    model.add(Conv1D(params['filters'],\n",
    "                     params['kernel_size'],\n",
    "                     padding='valid',\n",
    "                     activation=params['activation'],\n",
    "                     strides=params['strides']))\n",
    "    model.add(MaxPooling1D(pool_size=params['pool_size']))\n",
    "\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(LSTM(params['lstm_output_size']))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(params['last_activation']))\n",
    "\n",
    "    model.compile(loss=params['loss'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_phase = {\n",
    "    'kernel_size': 32,\n",
    "    'strides': 4,\n",
    "    'pool_size': 2,\n",
    "    'filters': 8,\n",
    "    'lstm_output_size': 236,\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'dropout': 0.09,\n",
    "    'activation': 'relu',\n",
    "    'optimizer': 'Nadam',\n",
    "    'last_activation': 'sigmoid'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_br_hb = {\n",
    "    'kernel_size': 2,\n",
    "    'strides': 1,\n",
    "    'pool_size': 1,\n",
    "    'filters': 2,\n",
    "    'lstm_output_size': 4,\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'dropout': 0.09,\n",
    "    'activation': 'relu',\n",
    "    'optimizer': 'Nadam',\n",
    "    'last_activation': 'sigmoid'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'kernel_size': 8,\n",
    "    'filters': 3,\n",
    "    'strides': 2,\n",
    "    'pool_size': 2,\n",
    "    'dropout': 0.09,\n",
    "    'optimizer': 'Nadam',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'activation': 'relu',\n",
    "    'last_activation': 'sigmoid',\n",
    "    'lstm_output_size': 256,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5 ; time elapsed: 0:00:00.006981\n",
      "iteration: 2 of 5 ; time elapsed: 0:01:33.252994\n",
      "iteration: 3 of 5 ; time elapsed: 0:03:14.546763\n",
      "iteration: 4 of 5 ; time elapsed: 0:05:02.364157\n",
      "iteration: 5 of 5 ; time elapsed: 0:06:55.987233\n",
      "Completed! Time elapsed: 0:09:01.107904\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['LSTM'] = {}\n",
    "accs = accuracies['LSTM']\n",
    "accs['phase'] = []\n",
    "accs['breathing'] = []\n",
    "accs['heartbeat'] = []\n",
    "accs['combined br hb'] = []\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "accs['test id'] = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # leave out person out validation\n",
    "    for ident in range(n):\n",
    "\n",
    "        # print current iteration and time elapsed from start\n",
    "        print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "        ## ----- Data preparation:\n",
    "        # Split the data\n",
    "        train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "        validation_idents = [idents[ident]]\n",
    "        test_idents = [idents[ident-1]]\n",
    "\n",
    "        # save test id to see which id yielded which accuracies\n",
    "        accs['test id'].append(test_idents[0])\n",
    "        \n",
    "        # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "        xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hr(path, train_idents, seconds)\n",
    "        xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hr(path, validation_idents, seconds)\n",
    "        xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hr(path, test_idents, seconds)\n",
    "\n",
    "        # Scale data with standard scaler then MinMax scaler\n",
    "        # Raw Phase data:\n",
    "        xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted breathing data:\n",
    "        br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted Heartbeat data:\n",
    "        hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "        cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify without autoencoders:\n",
    "        # Phase classifier:\n",
    "        model = LSTM_classifier_base(params_phase)\n",
    "        # reshape data for the classifier\n",
    "        xtt_train = xt_train.reshape(-1, xt_train[0].shape[0], 1)\n",
    "        xtt_valid = xt_valid.reshape(-1, xt_valid[0].shape[0], 1)\n",
    "        xtt_test = xt_test.reshape(-1, xt_test[0].shape[0], 1)\n",
    "        # train and evaluate\n",
    "        sc, curr_acc, epoch_data = model_train(model, xtt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xtt_valid, y_valid, xtt_test, y_test)\n",
    "        accs['phase'].append(curr_acc)\n",
    "\n",
    "        # Breathing classifier:\n",
    "        model = LSTM_classifier_base(params_br_hb)\n",
    "        # reshape data for the classifier\n",
    "        brt_train = br_train.reshape(-1, br_train[0].shape[0], 1)\n",
    "        brt_valid = br_valid.reshape(-1, br_valid[0].shape[0], 1)\n",
    "        brt_test = br_test.reshape(-1, br_test[0].shape[0], 1)\n",
    "        # train and evaluate\n",
    "        sc, curr_acc, epoch_data = model_train(model, brt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               brt_valid, y_valid, brt_test, y_test)\n",
    "        accs['breathing'].append(curr_acc)\n",
    "\n",
    "        # Heartbeat classifier:\n",
    "        model = LSTM_classifier_base(params_br_hb)\n",
    "        # reshape data for the classifier\n",
    "        hbt_train = hb_train.reshape(-1, hb_train[0].shape[0], 1)\n",
    "        hbt_valid = hb_valid.reshape(-1, hb_valid[0].shape[0], 1)\n",
    "        hbt_test = hb_test.reshape(-1, hb_test[0].shape[0], 1)\n",
    "        # train and evaluate\n",
    "        sc, curr_acc, epoch_data = model_train(model, hbt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               hbt_valid, y_valid, hbt_test, y_test)\n",
    "        accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "        # Combined classifier:\n",
    "        model = LSTM_classifier_base(params_br_hb)\n",
    "        # reshape data for the classifier\n",
    "        cmbt_train = cmb_train.reshape(-1, cmb_train[0].shape[0], 1)\n",
    "        cmbt_valid = cmb_valid.reshape(-1, cmb_valid[0].shape[0], 1)\n",
    "        cmbt_test = cmb_test.reshape(-1, cmb_test[0].shape[0], 1)\n",
    "        # train and evaluate\n",
    "        sc, curr_acc, epoch_data = model_train(model, cmbt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               cmbt_valid, y_valid, cmbt_test, y_test)\n",
    "        accs['combined br hb'].append(curr_acc)\n",
    "\n",
    "        \n",
    "        \n",
    "        ## ----- Classify with autoencoders:\n",
    "        # AE Training params\n",
    "        batch_size = 256\n",
    "        epochs = 100\n",
    "\n",
    "        # undercomplete AE\n",
    "        autoencoder, encoded = undercomplete_ae(xt_train, 60)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = LSTM_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "        # sparse AE\n",
    "        autoencoder, encoded = sparse_ae(xt_train, 60)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = LSTM_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['sparse'].append(curr_acc)\n",
    "\n",
    "        # deep AE\n",
    "        autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512])\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = LSTM_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['deep'].append(curr_acc)\n",
    "\n",
    "        # contractive AE\n",
    "        autoencoder, encoded = contractive_ae(xt_train, 60)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = LSTM_classifier(encoded, params)\n",
    "        model = Model(inputs=autoencoder.inputs, outputs=model)\n",
    "        model.compile(loss=params['loss'],\n",
    "                      optimizer=params['optimizer'],\n",
    "                      metrics=metrics)\n",
    "        sc, curr_acc, epoch_data = model_train(model, xt_train, y_train, params['batch_size'], params['epochs'],\n",
    "                                               xt_valid, y_valid, xt_test, y_test)\n",
    "        accs['contractive'].append(curr_acc)\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed!\", \"Time elapsed:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-144a3f730237>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# print accuracies of each method and corresponding id which yielded that accuracy (same row)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"only recognize index or columns for orient\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m     def to_numpy(\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.6333333\n",
      "- max: 0.9166667\n",
      "- mean: 0.7763399\n",
      "- median: 0.7941176\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.44117647\n",
      "- max: 0.6944444\n",
      "- mean: 0.5782026\n",
      "- median: 0.56666666\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.32352942\n",
      "- max: 0.5277778\n",
      "- mean: 0.42692813\n",
      "- median: 0.41666666\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.38235295\n",
      "- max: 0.5555556\n",
      "- mean: 0.47336602\n",
      "- median: 0.47058824\n",
      "\n",
      "undercomplete accuracies:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-ed147164b295>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# print some statistics for each method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint_accs_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-82-6c602642c01a>\u001b[0m in \u001b[0;36mprint_accs_stats\u001b[1;34m(accs)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# calculate and print some statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"accuracies:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"- min:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"- max:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"- mean:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2791\u001b[0m     \"\"\"\n\u001b[0;32m   2792\u001b[0m     return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n\u001b[1;32m-> 2793\u001b[1;33m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miscellanious\\Anaconda\\envs\\py37talos\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper loop function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper loop funciton for the sklearn and XGBoost classifiers\n",
    "def helper_loop(classifier_function, idents, n=5):\n",
    "    #returns a dictionary with accuracies\n",
    "\n",
    "    # set the variables in the dictionary\n",
    "    accs = {}\n",
    "    accs['phase'] = []\n",
    "    accs['breathing'] = []\n",
    "    accs['heartbeat'] = []\n",
    "    accs['combined br hb'] = []\n",
    "    accs['undercomplete'] = []\n",
    "    accs['sparse'] = []\n",
    "    accs['deep'] = []\n",
    "    accs['contractive'] = []\n",
    "    accs['test id'] = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        # leave out person out validation\n",
    "        for ident in range(n):\n",
    "\n",
    "            # print current iteration and time elapsed from start\n",
    "            print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "            ## ----- Data preparation:\n",
    "            # Split the data\n",
    "            train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "            validation_idents = [idents[ident]]\n",
    "            test_idents = [idents[ident-1]]\n",
    "\n",
    "            # save test id to see which id yielded which accuracies\n",
    "            accs['test id'].append(test_idents[0])\n",
    "\n",
    "            # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "            xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hr(path, train_idents, seconds)\n",
    "            xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hr(path, validation_idents, seconds)\n",
    "            xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hr(path, test_idents, seconds)\n",
    "\n",
    "            # change the y arrays to flat 1d arrays\n",
    "            y_train = y_train.ravel()\n",
    "            y_valid = y_valid.ravel()\n",
    "            y_test = y_test.ravel()\n",
    "            \n",
    "            # Scale data with standard scaler then MinMax scaler\n",
    "            # Raw Phase data:\n",
    "            xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "            # Hand extracted breathing data:\n",
    "            br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "            # Hand extracted Heartbeat data:\n",
    "            hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "            # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "            cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "\n",
    "\n",
    "\n",
    "            ## ----- Classify without autoencoders:\n",
    "            # Phase classifier:\n",
    "            model = classifier_function()\n",
    "            model.fit(xt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xt_test) == y_test) / y_test.shape[0]\n",
    "            accs['phase'].append(curr_acc)\n",
    "\n",
    "            # Breathing classifier:\n",
    "            base_model = classifier_function()\n",
    "            base_model.fit(br_train, y_train)\n",
    "            curr_acc = np.sum(base_model.predict(br_valid) == y_valid) / y_test.shape[0]\n",
    "            accs['breathing'].append(curr_acc)\n",
    "\n",
    "            # Heartbeat classifier:\n",
    "            base_model = classifier_function()\n",
    "            base_model.fit(hb_train, y_train)\n",
    "            curr_acc = np.sum(base_model.predict(hb_valid) == y_valid) / y_test.shape[0]\n",
    "            accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "            # Combined classifier:\n",
    "            base_model = classifier_function()\n",
    "            base_model.fit(cmb_train, y_train)\n",
    "            curr_acc = np.sum(base_model.predict(cmb_valid) == y_valid) / y_test.shape[0]\n",
    "            accs['combined br hb'].append(curr_acc)\n",
    "\n",
    "\n",
    "\n",
    "            ## ----- Classify with autoencoders:\n",
    "            # AE Training params\n",
    "            batch_size = 256\n",
    "            epochs = 100\n",
    "\n",
    "            # undercomplete AE\n",
    "            autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "            sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                                   xt_valid, xt_valid, xt_test, xt_test)\n",
    "            model = classifier_function()\n",
    "            xtt_train = encoded.predict(xt_train)\n",
    "            xtt_test = encoded.predict(xt_test)\n",
    "            model.fit(xtt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xtt_test) == y_test) / y_test.shape[0]\n",
    "            accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "            # sparse AE\n",
    "            autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "            sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                                   xt_valid, xt_valid, xt_test, xt_test)\n",
    "            model = classifier_function()\n",
    "            xtt_train = encoded.predict(xt_train)\n",
    "            xtt_test = encoded.predict(xt_test)\n",
    "            model.fit(xtt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xtt_test) == y_test) / y_test.shape[0]\n",
    "            accs['sparse'].append(curr_acc)\n",
    "\n",
    "            # deep AE\n",
    "            autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "            sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                                   xt_valid, xt_valid, xt_test, xt_test)\n",
    "            model = classifier_function()\n",
    "            xtt_train = encoded.predict(xt_train)\n",
    "            xtt_test = encoded.predict(xt_test)\n",
    "            model.fit(xtt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xtt_test) == y_test) / y_test.shape[0]\n",
    "            accs['deep'].append(curr_acc)\n",
    "\n",
    "            # contractive AE\n",
    "            autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "            sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                                   xt_valid, xt_valid, xt_test, xt_test)\n",
    "            model = classifier_function()\n",
    "            xtt_train = encoded.predict(xt_train)\n",
    "            xtt_test = encoded.predict(xt_test)\n",
    "            model.fit(xtt_train, y_train)\n",
    "            curr_acc = np.sum(model.predict(xtt_test) == y_test) / y_test.shape[0]\n",
    "            accs['contractive'].append(curr_acc)\n",
    "\n",
    "    # Print total time required to run this\n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Completed!\", \"Time elapsed:\", elapsed_time)\n",
    "    \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def KNN_classifier():\n",
    "    model = KNeighborsClassifier(p=3, n_neighbors=7, metric='cosine')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 1 ; time elapsed: 0:00:00.006981\n",
      "Completed! Time elapsed: 0:01:42.146120\n"
     ]
    }
   ],
   "source": [
    "accs = helper_loop(KNN_classifier, idents, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies['kNN'] = accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5 ; time elapsed: 0:00:00.006980\n",
      "iteration: 2 of 5 ; time elapsed: 0:01:28.628020\n",
      "iteration: 3 of 5 ; time elapsed: 0:03:01.400610\n",
      "iteration: 4 of 5 ; time elapsed: 0:04:38.843199\n",
      "iteration: 5 of 5 ; time elapsed: 0:06:21.714417\n",
      "Completed! Time elapsed: 0:08:09.508184\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['kNN'] = {}\n",
    "accs = accuracies['kNN']\n",
    "accs['phase'] = []\n",
    "accs['breathing'] = []\n",
    "accs['heartbeat'] = []\n",
    "accs['combined br hb'] = []\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "accs['test id'] = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # leave out person out validation\n",
    "    for ident in range(n):\n",
    "\n",
    "        # print current iteration and time elapsed from start\n",
    "        print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "        ## ----- Data preparation:\n",
    "        # Split the data\n",
    "        train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "        validation_idents = [idents[ident]]\n",
    "        test_idents = [idents[ident-1]]\n",
    "        \n",
    "        # save test id to see which id yielded which accuracies\n",
    "        accs['test id'].append(test_idents[0])\n",
    "\n",
    "        # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "        xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hr(path, train_idents, seconds)\n",
    "        xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hr(path, validation_idents, seconds)\n",
    "        xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hr(path, test_idents, seconds)\n",
    "\n",
    "        # Scale data with standard scaler then MinMax scaler\n",
    "        # Raw Phase data:\n",
    "        xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted breathing data:\n",
    "        br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted Heartbeat data:\n",
    "        hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "        cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify without autoencoders:\n",
    "        # Phase classifier:\n",
    "        model = KNN_classifier()\n",
    "        model.fit(xt_train, y_train.ravel())\n",
    "        curr_acc = np.sum(model.predict(xt_test) == y_test.ravel()) / y_test.shape[0]\n",
    "        accs['phase'].append(curr_acc)\n",
    "\n",
    "        # Breathing classifier:\n",
    "        base_model = KNN_classifier()\n",
    "        base_model.fit(br_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(br_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['breathing'].append(curr_acc)\n",
    "\n",
    "        # Heartbeat classifier:\n",
    "        base_model = KNN_classifier()\n",
    "        base_model.fit(hb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(hb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "        # Combined classifier:\n",
    "        base_model = KNN_classifier()\n",
    "        base_model.fit(cmb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(cmb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['combined br hb'].append(curr_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify with autoencoders:\n",
    "        # AE Training params\n",
    "        batch_size = 256\n",
    "        epochs = 100\n",
    "\n",
    "        # undercomplete AE\n",
    "        autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = KNN_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "        # sparse AE\n",
    "        autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = KNN_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['sparse'].append(curr_acc)\n",
    "\n",
    "        # deep AE\n",
    "        autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = KNN_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['deep'].append(curr_acc)\n",
    "\n",
    "        # contractive AE\n",
    "        autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = KNN_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['contractive'].append(curr_acc)\n",
    "\n",
    "# Print total time required to run this\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed!\", \"Time elapsed:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>breathing</th>\n",
       "      <th>heartbeat</th>\n",
       "      <th>combined br hb</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "      <th>test id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>62i9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>2gu87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>iz2ps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>1mpau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>7dwjy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  breathing  heartbeat  combined br hb  undercomplete    sparse  \\\n",
       "0  0.583333   0.875000   0.583333        0.916667       0.750000  0.791667   \n",
       "1  0.588235   0.970588   0.676471        0.941176       0.705882  0.617647   \n",
       "2  0.611111   0.666667   0.583333        0.638889       0.777778  0.722222   \n",
       "3  0.588235   0.529412   0.294118        0.411765       0.647059  0.588235   \n",
       "4  0.766667   0.900000   0.533333        0.700000       0.733333  0.733333   \n",
       "\n",
       "       deep  contractive test id  \n",
       "0  0.708333     0.583333   62i9y  \n",
       "1  0.676471     0.764706   2gu87  \n",
       "2  0.555556     0.611111   iz2ps  \n",
       "3  0.441176     0.647059   1mpau  \n",
       "4  0.500000     0.600000   7dwjy  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase accuracies:\n",
      "- min: 0.5833333333333334\n",
      "- max: 0.7666666666666667\n",
      "- mean: 0.627516339869281\n",
      "- median: 0.5882352941176471\n",
      "\n",
      "breathing accuracies:\n",
      "- min: 0.5294117647058824\n",
      "- max: 0.9705882352941176\n",
      "- mean: 0.7883333333333333\n",
      "- median: 0.875\n",
      "\n",
      "heartbeat accuracies:\n",
      "- min: 0.29411764705882354\n",
      "- max: 0.6764705882352942\n",
      "- mean: 0.5341176470588235\n",
      "- median: 0.5833333333333334\n",
      "\n",
      "combined br hb accuracies:\n",
      "- min: 0.4117647058823529\n",
      "- max: 0.9411764705882353\n",
      "- mean: 0.7216993464052287\n",
      "- median: 0.7\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.6470588235294118\n",
      "- max: 0.7777777777777778\n",
      "- mean: 0.7228104575163399\n",
      "- median: 0.7333333333333333\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5882352941176471\n",
      "- max: 0.7916666666666666\n",
      "- mean: 0.6906209150326797\n",
      "- median: 0.7222222222222222\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.4411764705882353\n",
      "- max: 0.7083333333333334\n",
      "- mean: 0.5763071895424836\n",
      "- median: 0.5555555555555556\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5833333333333334\n",
      "- max: 0.7647058823529411\n",
      "- mean: 0.6412418300653595\n",
      "- median: 0.6111111111111112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def SVC_classifier():\n",
    "    model = SVC(kernel='rbf', C=1.5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5 ; time elapsed: 0:00:00.007978\n",
      "iteration: 2 of 5 ; time elapsed: 0:01:57.303690\n",
      "iteration: 3 of 5 ; time elapsed: 0:03:53.245221\n",
      "iteration: 4 of 5 ; time elapsed: 0:05:56.133007\n",
      "iteration: 5 of 5 ; time elapsed: 0:08:10.454109\n",
      "Completed! Time elapsed: 0:10:35.452079\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['SVC'] = {}\n",
    "accs = accuracies['SVC']\n",
    "accs['phase'] = []\n",
    "accs['breathing'] = []\n",
    "accs['heartbeat'] = []\n",
    "accs['combined br hb'] = []\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "accs['test id'] = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # leave out person out validation\n",
    "    for ident in range(n):\n",
    "\n",
    "        # print current iteration and time elapsed from start\n",
    "        print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "        ## ----- Data preparation:\n",
    "        # Split the data\n",
    "        train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "        validation_idents = [idents[ident]]\n",
    "        test_idents = [idents[ident-1]]\n",
    "        \n",
    "        # save test id to see which id yielded which accuracies\n",
    "        accs['test id'].append(test_idents[0])\n",
    "\n",
    "        # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "        xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hr(path, train_idents, seconds)\n",
    "        xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hr(path, validation_idents, seconds)\n",
    "        xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hr(path, test_idents, seconds)\n",
    "\n",
    "        # Scale data with standard scaler then MinMax scaler\n",
    "        # Raw Phase data:\n",
    "        xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted breathing data:\n",
    "        br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted Heartbeat data:\n",
    "        hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "        cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify without autoencoders:\n",
    "        # Phase classifier:\n",
    "        model = SVC_classifier()\n",
    "        model.fit(xt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['phase'].append(curr_acc)\n",
    "\n",
    "        # Breathing classifier:\n",
    "        base_model = SVC_classifier()\n",
    "        base_model.fit(br_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(br_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['breathing'].append(curr_acc)\n",
    "\n",
    "        # Heartbeat classifier:\n",
    "        base_model = SVC_classifier()\n",
    "        base_model.fit(hb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(hb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "        # Combined classifier:\n",
    "        base_model = SVC_classifier()\n",
    "        base_model.fit(cmb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(cmb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['combined br hb'].append(curr_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify with autoencoders:\n",
    "        # AE Training params\n",
    "        batch_size = 256\n",
    "        epochs = 100\n",
    "\n",
    "        # undercomplete AE\n",
    "        autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = SVC_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "        # sparse AE\n",
    "        autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = SVC_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['sparse'].append(curr_acc)\n",
    "\n",
    "        # deep AE\n",
    "        autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = SVC_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['deep'].append(curr_acc)\n",
    "\n",
    "        # contractive AE\n",
    "        autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = SVC_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['contractive'].append(curr_acc)\n",
    "\n",
    "# Print total time required to run this\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed!\", \"Time elapsed:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       base  undercomplete    sparse      deep  contractive\n",
       "0  0.750000       0.714286  0.642857  0.607143     0.642857\n",
       "1  0.705882       0.588235  0.676471  0.529412     0.588235\n",
       "2  0.666667       0.527778  0.555556  0.611111     0.583333\n",
       "3  0.470588       0.500000  0.500000  0.470588     0.500000\n",
       "4  0.600000       0.600000  0.533333  0.666667     0.600000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print accuracies of each method and corresponding id which yielded that accuracy (same row)\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base accuracies:\n",
      "- min: 0.47058823529411764\n",
      "- max: 0.75\n",
      "- mean: 0.6386274509803922\n",
      "- median: 0.6666666666666666\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.7142857142857143\n",
      "- mean: 0.5860597572362278\n",
      "- median: 0.5882352941176471\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.6764705882352942\n",
      "- mean: 0.5816433239962653\n",
      "- median: 0.5555555555555556\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.47058823529411764\n",
      "- max: 0.6666666666666666\n",
      "- mean: 0.5769841269841269\n",
      "- median: 0.6071428571428571\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.6428571428571429\n",
      "- mean: 0.5828851540616247\n",
      "- median: 0.5882352941176471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some statistics for each method\n",
    "print_accs_stats(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def random_forest_classifier():\n",
    "    model = RandomForestClassifier(n_estimators = 250,\n",
    "                                     min_samples_split = 10,\n",
    "                                     min_samples_leaf = 4,\n",
    "                                     max_features = 'auto',\n",
    "                                     max_depth = 90,\n",
    "                                     bootstrap = True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5 ; time elapsed: 0:00:00.004987\n",
      "iteration: 2 of 5 ; time elapsed: 0:02:09.820573\n",
      "iteration: 3 of 5 ; time elapsed: 0:04:27.855148\n",
      "iteration: 4 of 5 ; time elapsed: 0:06:45.959286\n",
      "iteration: 5 of 5 ; time elapsed: 0:09:09.360952\n",
      "Completed! Time elapsed: 0:11:51.139776\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['random_forest'] = {}\n",
    "accs = accuracies['random_forest']\n",
    "accs['phase'] = []\n",
    "accs['breathing'] = []\n",
    "accs['heartbeat'] = []\n",
    "accs['combined br hb'] = []\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "accs['test id'] = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # leave out person out validation\n",
    "    for ident in range(n):\n",
    "\n",
    "        # print current iteration and time elapsed from start\n",
    "        print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "        ## ----- Data preparation:\n",
    "        # Split the data\n",
    "        train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "        validation_idents = [idents[ident]]\n",
    "        test_idents = [idents[ident-1]]\n",
    "        \n",
    "        # save test id to see which id yielded which accuracies\n",
    "        accs['test id'].append(test_idents[0])\n",
    "\n",
    "        # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "        xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hr(path, train_idents, seconds)\n",
    "        xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hr(path, validation_idents, seconds)\n",
    "        xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hr(path, test_idents, seconds)\n",
    "\n",
    "        # Scale data with standard scaler then MinMax scaler\n",
    "        # Raw Phase data:\n",
    "        xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted breathing data:\n",
    "        br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted Heartbeat data:\n",
    "        hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "        cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify without autoencoders:\n",
    "        # Phase classifier:\n",
    "        model = random_forest_classifier()\n",
    "        model.fit(xt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['phase'].append(curr_acc)\n",
    "\n",
    "        # Breathing classifier:\n",
    "        base_model = random_forest_classifier()\n",
    "        base_model.fit(br_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(br_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['breathing'].append(curr_acc)\n",
    "\n",
    "        # Heartbeat classifier:\n",
    "        base_model = random_forest_classifier()\n",
    "        base_model.fit(hb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(hb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "        # Combined classifier:\n",
    "        base_model = random_forest_classifier()\n",
    "        base_model.fit(cmb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(cmb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['combined br hb'].append(curr_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify with autoencoders:\n",
    "        # AE Training params\n",
    "        batch_size = 256\n",
    "        epochs = 100\n",
    "\n",
    "        # undercomplete AE\n",
    "        autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = random_forest_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "        # sparse AE\n",
    "        autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = random_forest_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['sparse'].append(curr_acc)\n",
    "\n",
    "        # deep AE\n",
    "        autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = random_forest_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['deep'].append(curr_acc)\n",
    "\n",
    "        # contractive AE\n",
    "        autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = random_forest_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['contractive'].append(curr_acc)\n",
    "\n",
    "# Print total time required to run this\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed!\", \"Time elapsed:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       base  undercomplete    sparse      deep  contractive\n",
       "0  1.000000       1.000000  0.928571  0.857143     1.000000\n",
       "1  0.647059       0.705882  0.735294  0.647059     0.735294\n",
       "2  0.777778       0.805556  0.833333  0.777778     0.805556\n",
       "3  0.705882       0.617647  0.705882  0.411765     0.588235\n",
       "4  0.566667       0.500000  0.566667  0.566667     0.733333"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base accuracies:\n",
      "- min: 0.5666666666666667\n",
      "- max: 1.0\n",
      "- mean: 0.7394771241830066\n",
      "- median: 0.7058823529411765\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.5\n",
      "- max: 1.0\n",
      "- mean: 0.7258169934640523\n",
      "- median: 0.7058823529411765\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5666666666666667\n",
      "- max: 0.9285714285714286\n",
      "- mean: 0.7539495798319328\n",
      "- median: 0.7352941176470589\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.4117647058823529\n",
      "- max: 0.8571428571428571\n",
      "- mean: 0.6520821661998133\n",
      "- median: 0.6470588235294118\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5882352941176471\n",
      "- max: 1.0\n",
      "- mean: 0.772483660130719\n",
      "- median: 0.7352941176470589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "def naive_bayesian_classifier():\n",
    "    model = ComplementNB()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5 ; time elapsed: 0:00:00.005958\n",
      "iteration: 2 of 5 ; time elapsed: 0:02:10.568915\n",
      "iteration: 3 of 5 ; time elapsed: 0:04:29.500474\n",
      "iteration: 4 of 5 ; time elapsed: 0:06:52.040741\n",
      "iteration: 5 of 5 ; time elapsed: 0:09:30.329344\n",
      "Completed! Time elapsed: 0:12:25.619719\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['naive_bayesian'] = {}\n",
    "accs = accuracies['naive_bayesian']\n",
    "accs['phase'] = []\n",
    "accs['breathing'] = []\n",
    "accs['heartbeat'] = []\n",
    "accs['combined br hb'] = []\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "accs['test id'] = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # leave out person out validation\n",
    "    for ident in range(n):\n",
    "\n",
    "        # print current iteration and time elapsed from start\n",
    "        print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "        ## ----- Data preparation:\n",
    "        # Split the data\n",
    "        train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "        validation_idents = [idents[ident]]\n",
    "        test_idents = [idents[ident-1]]\n",
    "        \n",
    "        # save test id to see which id yielded which accuracies\n",
    "        accs['test id'].append(test_idents[0])\n",
    "\n",
    "        # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "        xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hr(path, train_idents, seconds)\n",
    "        xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hr(path, validation_idents, seconds)\n",
    "        xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hr(path, test_idents, seconds)\n",
    "\n",
    "        # Scale data with standard scaler then MinMax scaler\n",
    "        # Raw Phase data:\n",
    "        xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted breathing data:\n",
    "        br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted Heartbeat data:\n",
    "        hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "        cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify without autoencoders:\n",
    "        # Phase classifier:\n",
    "        model = naive_bayesian_classifier()\n",
    "        model.fit(xt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['phase'].append(curr_acc)\n",
    "\n",
    "        # Breathing classifier:\n",
    "        base_model = naive_bayesian_classifier()\n",
    "        base_model.fit(br_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(br_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['breathing'].append(curr_acc)\n",
    "\n",
    "        # Heartbeat classifier:\n",
    "        base_model = naive_bayesian_classifier()\n",
    "        base_model.fit(hb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(hb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "        # Combined classifier:\n",
    "        base_model = naive_bayesian_classifier()\n",
    "        base_model.fit(cmb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(cmb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['combined br hb'].append(curr_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify with autoencoders:\n",
    "        # AE Training params\n",
    "        batch_size = 256\n",
    "        epochs = 100\n",
    "\n",
    "        # undercomplete AE\n",
    "        autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "    #     print(\"undercomplete AUTOENCODER TRAINING: \", ident)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = naive_bayesian_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "    #     print(\"undercomplete CLASSIFICATION TRAINING: \", ident)\n",
    "        model.fit(xtt_train, y_train.ravel())\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.ravel()) / y_test.shape[0]\n",
    "        accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "        # sparse AE\n",
    "        autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "    #     print(\"sparse AUTOENCODER TRAINING: \", ident)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = naive_bayesian_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "    #     print(\"sparse CLASSIFICATION TRAINING: \", ident)\n",
    "        model.fit(xtt_train, y_train.ravel())\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.ravel()) / y_test.shape[0]\n",
    "        accs['sparse'].append(curr_acc)\n",
    "\n",
    "        # deep AE\n",
    "        autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "    #     print(\"deep AUTOENCODER TRAINING: \", ident)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = naive_bayesian_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "    #     print(\"deep CLASSIFICATION TRAINING: \", ident)\n",
    "        model.fit(xtt_train, y_train.ravel())\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.ravel()) / y_test.shape[0]\n",
    "        accs['deep'].append(curr_acc)\n",
    "\n",
    "        # contractive AE\n",
    "        autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "    #     print(\"contractive AUTOENCODER TRAINING: \", ident)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = naive_bayesian_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "    #     print(\"contractive CLASSIFICATION TRAINING: \", ident)\n",
    "        model.fit(xtt_train, y_train.ravel())\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.ravel()) / y_test.shape[0]\n",
    "        accs['contractive'].append(curr_acc)\n",
    "\n",
    "# Print total time required to run this\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed!\", \"Time elapsed:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       base  undercomplete    sparse      deep  contractive\n",
       "0  0.500000       0.571429  0.571429  0.535714     0.535714\n",
       "1  0.382353       0.500000  0.558824  0.500000     0.500000\n",
       "2  0.361111       0.500000  0.500000  0.555556     0.472222\n",
       "3  0.588235       0.529412  0.529412  0.529412     0.470588\n",
       "4  0.433333       0.566667  0.566667  0.533333     0.466667"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base accuracies:\n",
      "- min: 0.3611111111111111\n",
      "- max: 0.5882352941176471\n",
      "- mean: 0.4530065359477124\n",
      "- median: 0.43333333333333335\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.5714285714285714\n",
      "- mean: 0.5335014005602241\n",
      "- median: 0.5294117647058824\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.5714285714285714\n",
      "- mean: 0.545266106442577\n",
      "- median: 0.5588235294117647\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5\n",
      "- max: 0.5555555555555556\n",
      "- mean: 0.5308029878618113\n",
      "- median: 0.5333333333333333\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.4666666666666667\n",
      "- max: 0.5357142857142857\n",
      "- mean: 0.4890382819794585\n",
      "- median: 0.4722222222222222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGBoost_classifier():\n",
    "    model = XGBClassifier(n_estimators = 83)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the autoencoders with the classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of 5 ; time elapsed: 0:00:00.006981\n",
      "iteration: 2 of 5 ; time elapsed: 0:02:24.570867\n",
      "iteration: 3 of 5 ; time elapsed: 0:04:54.471335\n",
      "iteration: 4 of 5 ; time elapsed: 0:07:31.323163\n",
      "iteration: 5 of 5 ; time elapsed: 0:10:13.582053\n",
      "Completed! Time elapsed: 0:13:16.091685\n"
     ]
    }
   ],
   "source": [
    "# set the variables in the dictionary\n",
    "accuracies['XGBoost'] = {}\n",
    "accs = accuracies['XGBoost']\n",
    "accs['phase'] = []\n",
    "accs['breathing'] = []\n",
    "accs['heartbeat'] = []\n",
    "accs['combined br hb'] = []\n",
    "accs['undercomplete'] = []\n",
    "accs['sparse'] = []\n",
    "accs['deep'] = []\n",
    "accs['contractive'] = []\n",
    "accs['test id'] = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # leave out person out validation\n",
    "    for ident in range(n):\n",
    "\n",
    "        # print current iteration and time elapsed from start\n",
    "        print(\"iteration:\", ident+1, \"of\", n, \"; time elapsed:\", datetime.now()-start_time)\n",
    "\n",
    "        ## ----- Data preparation:\n",
    "        # Split the data\n",
    "        train_idents = [x for i, x in enumerate(idents) if (i != ident and i != (n-1+ident)%n)]\n",
    "        validation_idents = [idents[ident]]\n",
    "        test_idents = [idents[ident-1]]\n",
    "        \n",
    "        # save test id to see which id yielded which accuracies\n",
    "        accs['test id'].append(test_idents[0])\n",
    "\n",
    "        # Load data (xt-raw phase data, y-class, br-breathing data, hb-heartbeat data, cmb-combined [br,hb])\n",
    "        xt_train, y_train, br_train, hb_train, cmb_train = get_data_from_idents_br_hr(path, train_idents, seconds)\n",
    "        xt_valid, y_valid, br_valid, hb_valid, cmb_valid = get_data_from_idents_br_hr(path, validation_idents, seconds)\n",
    "        xt_test, y_test, br_test, hb_test, cmb_test = get_data_from_idents_br_hr(path, test_idents, seconds)\n",
    "\n",
    "        # Scale data with standard scaler then MinMax scaler\n",
    "        # Raw Phase data:\n",
    "        xt_train, xt_valid, xt_test = scale_data(xt_train, xt_valid, xt_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted breathing data:\n",
    "        br_train, br_valid, br_test = scale_data(br_train, br_valid, br_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Hand extracted Heartbeat data:\n",
    "        hb_train, hb_valid, hb_test = scale_data(hb_train, hb_valid, hb_test, standardScaler=True, minMaxScaler=True)\n",
    "        # Combined breathing and heartbeat data (joined together into one matrix)\n",
    "        cmb_train, cmb_valid, cmb_test = scale_data(cmb_train, cmb_valid, cmb_test, standardScaler=True, minMaxScaler=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify without autoencoders:\n",
    "        # Phase classifier:\n",
    "        model = random_forest_classifier()\n",
    "        model.fit(xt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['phase'].append(curr_acc)\n",
    "\n",
    "        # Breathing classifier:\n",
    "        base_model = random_forest_classifier()\n",
    "        base_model.fit(br_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(br_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['breathing'].append(curr_acc)\n",
    "\n",
    "        # Heartbeat classifier:\n",
    "        base_model = random_forest_classifier()\n",
    "        base_model.fit(hb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(hb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['heartbeat'].append(curr_acc)\n",
    "\n",
    "        # Combined classifier:\n",
    "        base_model = random_forest_classifier()\n",
    "        base_model.fit(cmb_train, y_train.ravel())\n",
    "        curr_acc = np.sum(base_model.predict(cmb_valid) == y_valid.ravel()) / y_test.shape[0]\n",
    "        accs['combined br hb'].append(curr_acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## ----- Classify with autoencoders:\n",
    "        # AE Training params\n",
    "        batch_size = 256\n",
    "        epochs = 100\n",
    "\n",
    "        # undercomplete AE\n",
    "        autoencoder, encoded = undercomplete_ae(xt_train, 60, encoded_as_model=True)\n",
    "    #     print(\"undercomplete AUTOENCODER TRAINING: \", ident)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = XGBoost_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "    #     print(\"undercomplete CLASSIFICATION TRAINING: \", ident)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['undercomplete'].append(curr_acc)\n",
    "\n",
    "        # sparse AE\n",
    "        autoencoder, encoded = sparse_ae(xt_train, 60, encoded_as_model=True)\n",
    "    #     print(\"sparse AUTOENCODER TRAINING: \", ident)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = XGBoost_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "    #     print(\"sparse CLASSIFICATION TRAINING: \", ident)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['sparse'].append(curr_acc)\n",
    "\n",
    "        # deep AE\n",
    "        autoencoder, encoded = deep_ae(xt_train, enc_layers=[512,256], encoding_dim=60, dec_layers=[256,512], encoded_as_model=True)\n",
    "    #     print(\"deep AUTOENCODER TRAINING: \", ident)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = XGBoost_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "    #     print(\"deep CLASSIFICATION TRAINING: \", ident)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['deep'].append(curr_acc)\n",
    "\n",
    "        # contractive AE\n",
    "        autoencoder, encoded = contractive_ae(xt_train, 60, encoded_as_model=True)\n",
    "    #     print(\"contractive AUTOENCODER TRAINING: \", ident)\n",
    "        sc, curr_acc, epoch_data = model_train(autoencoder, xt_train, xt_train, batch_size, epochs,\n",
    "                                               xt_valid, xt_valid, xt_test, xt_test)\n",
    "        model = XGBoost_classifier()\n",
    "        xtt_train = encoded.predict(xt_train)\n",
    "        xtt_test = encoded.predict(xt_test)\n",
    "    #     print(\"contractive CLASSIFICATION TRAINING: \", ident)\n",
    "        model.fit(xtt_train, y_train)\n",
    "        curr_acc = np.sum(model.predict(xtt_test) == y_test.T) / y_test.shape[0]\n",
    "        accs['contractive'].append(curr_acc)\n",
    "\n",
    "# Print total time required to run this\n",
    "end_time = datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Completed!\", \"Time elapsed:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base</th>\n",
       "      <th>undercomplete</th>\n",
       "      <th>sparse</th>\n",
       "      <th>deep</th>\n",
       "      <th>contractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.617647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       base  undercomplete    sparse      deep  contractive\n",
       "0  1.000000       1.000000  1.000000  1.000000     1.000000\n",
       "1  0.647059       0.735294  0.794118  0.617647     0.617647\n",
       "2  0.666667       0.722222  0.777778  0.666667     0.583333\n",
       "3  0.735294       0.617647  0.588235  0.617647     0.529412\n",
       "4  0.600000       0.566667  0.566667  0.566667     0.566667"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some statistics\n",
    "pandas.DataFrame.from_dict(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base accuracies:\n",
      "- min: 0.6\n",
      "- max: 1.0\n",
      "- mean: 0.7298039215686274\n",
      "- median: 0.6666666666666666\n",
      "\n",
      "undercomplete accuracies:\n",
      "- min: 0.5666666666666667\n",
      "- max: 1.0\n",
      "- mean: 0.7283660130718955\n",
      "- median: 0.7222222222222222\n",
      "\n",
      "sparse accuracies:\n",
      "- min: 0.5666666666666667\n",
      "- max: 1.0\n",
      "- mean: 0.745359477124183\n",
      "- median: 0.7777777777777778\n",
      "\n",
      "deep accuracies:\n",
      "- min: 0.5666666666666667\n",
      "- max: 1.0\n",
      "- mean: 0.6937254901960784\n",
      "- median: 0.6176470588235294\n",
      "\n",
      "contractive accuracies:\n",
      "- min: 0.5294117647058824\n",
      "- max: 1.0\n",
      "- mean: 0.6594117647058824\n",
      "- median: 0.5833333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in accs:\n",
    "    print(key, \"accuracies:\")\n",
    "    print(\"- min:\", np.min(accs[key]))\n",
    "    print(\"- max:\", np.max(accs[key]))\n",
    "    print(\"- mean:\", np.mean(accs[key]))\n",
    "    print(\"- median:\", np.median(accs[key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Compare Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print min, max, mean, median for each clasifier/autoencoder combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_dense:\n",
      "  base accuracies:\n",
      "   - min: 0.6\n",
      "   - max: 1.0\n",
      "   - mean: 0.7298039215686274\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7283660130718955\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.745359477124183\n",
      "   - median: 0.7777777777777778\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.6937254901960784\n",
      "   - median: 0.6176470588235294\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5294117647058824\n",
      "   - max: 1.0\n",
      "   - mean: 0.6594117647058824\n",
      "   - median: 0.5833333333333334\n",
      "\n",
      "\n",
      "\n",
      "LSTM:\n",
      "  base accuracies:\n",
      "   - min: 0.6\n",
      "   - max: 1.0\n",
      "   - mean: 0.7298039215686274\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7283660130718955\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.745359477124183\n",
      "   - median: 0.7777777777777778\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.6937254901960784\n",
      "   - median: 0.6176470588235294\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5294117647058824\n",
      "   - max: 1.0\n",
      "   - mean: 0.6594117647058824\n",
      "   - median: 0.5833333333333334\n",
      "\n",
      "\n",
      "\n",
      "kNN:\n",
      "  base accuracies:\n",
      "   - min: 0.6\n",
      "   - max: 1.0\n",
      "   - mean: 0.7298039215686274\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7283660130718955\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.745359477124183\n",
      "   - median: 0.7777777777777778\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.6937254901960784\n",
      "   - median: 0.6176470588235294\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5294117647058824\n",
      "   - max: 1.0\n",
      "   - mean: 0.6594117647058824\n",
      "   - median: 0.5833333333333334\n",
      "\n",
      "\n",
      "\n",
      "SVC:\n",
      "  base accuracies:\n",
      "   - min: 0.6\n",
      "   - max: 1.0\n",
      "   - mean: 0.7298039215686274\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7283660130718955\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.745359477124183\n",
      "   - median: 0.7777777777777778\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.6937254901960784\n",
      "   - median: 0.6176470588235294\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5294117647058824\n",
      "   - max: 1.0\n",
      "   - mean: 0.6594117647058824\n",
      "   - median: 0.5833333333333334\n",
      "\n",
      "\n",
      "\n",
      "random_forest:\n",
      "  base accuracies:\n",
      "   - min: 0.6\n",
      "   - max: 1.0\n",
      "   - mean: 0.7298039215686274\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7283660130718955\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.745359477124183\n",
      "   - median: 0.7777777777777778\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.6937254901960784\n",
      "   - median: 0.6176470588235294\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5294117647058824\n",
      "   - max: 1.0\n",
      "   - mean: 0.6594117647058824\n",
      "   - median: 0.5833333333333334\n",
      "\n",
      "\n",
      "\n",
      "naive_bayesian:\n",
      "  base accuracies:\n",
      "   - min: 0.6\n",
      "   - max: 1.0\n",
      "   - mean: 0.7298039215686274\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7283660130718955\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.745359477124183\n",
      "   - median: 0.7777777777777778\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.6937254901960784\n",
      "   - median: 0.6176470588235294\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5294117647058824\n",
      "   - max: 1.0\n",
      "   - mean: 0.6594117647058824\n",
      "   - median: 0.5833333333333334\n",
      "\n",
      "\n",
      "\n",
      "XGBoost:\n",
      "  base accuracies:\n",
      "   - min: 0.6\n",
      "   - max: 1.0\n",
      "   - mean: 0.7298039215686274\n",
      "   - median: 0.6666666666666666\n",
      "\n",
      "  undercomplete accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.7283660130718955\n",
      "   - median: 0.7222222222222222\n",
      "\n",
      "  sparse accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.745359477124183\n",
      "   - median: 0.7777777777777778\n",
      "\n",
      "  deep accuracies:\n",
      "   - min: 0.5666666666666667\n",
      "   - max: 1.0\n",
      "   - mean: 0.6937254901960784\n",
      "   - median: 0.6176470588235294\n",
      "\n",
      "  contractive accuracies:\n",
      "   - min: 0.5294117647058824\n",
      "   - max: 1.0\n",
      "   - mean: 0.6594117647058824\n",
      "   - median: 0.5833333333333334\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in accuracies:\n",
    "    print(classifier + \":\")\n",
    "    accs = accuracies[classifier]\n",
    "    for key in accuracies[classifier]:\n",
    "        print(\"  \" + key, \"accuracies:\")\n",
    "        print(\"   - min:\", np.min(accs[key]))\n",
    "        print(\"   - max:\", np.max(accs[key]))\n",
    "        print(\"   - mean:\", np.mean(accs[key]))\n",
    "        print(\"   - median:\", np.median(accs[key]))\n",
    "        print(\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all accuracies in table form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_dense:\n",
      "       base  undercomplete    sparse      deep  contractive\n",
      "0  0.642857       0.857143  0.857143  0.571429     0.785714\n",
      "1  0.676471       0.705882  0.764706  0.529412     0.558824\n",
      "2  0.694444       0.666667  0.694444  0.638889     0.611111\n",
      "3  0.411765       0.558824  0.617647  0.500000     0.470588\n",
      "4  0.500000       0.600000  0.800000  0.500000     0.700000\n",
      "\n",
      "\n",
      "LSTM:\n",
      "       base  undercomplete    sparse      deep  contractive\n",
      "0  0.785714       0.750000  0.857143  0.750000     0.892857\n",
      "1  0.764706       0.676471  0.705882  0.764706     0.735294\n",
      "2  0.777778       0.666667  0.694444  0.500000     0.666667\n",
      "3  0.676471       0.588235  0.588235  0.500000     0.647059\n",
      "4  0.666667       0.533333  0.633333  0.633333     0.666667\n",
      "\n",
      "\n",
      "kNN:\n",
      "       base  undercomplete    sparse      deep  contractive\n",
      "0  0.607143       0.750000  0.642857  0.714286     0.571429\n",
      "1  0.588235       0.705882  0.647059  0.735294     0.735294\n",
      "2  0.611111       0.694444  0.666667  0.555556     0.750000\n",
      "3  0.588235       0.588235  0.558824  0.500000     0.588235\n",
      "4  0.766667       0.600000  0.666667  0.733333     0.500000\n",
      "\n",
      "\n",
      "SVC:\n",
      "       base  undercomplete    sparse      deep  contractive\n",
      "0  0.750000       0.714286  0.642857  0.607143     0.642857\n",
      "1  0.705882       0.588235  0.676471  0.529412     0.588235\n",
      "2  0.666667       0.527778  0.555556  0.611111     0.583333\n",
      "3  0.470588       0.500000  0.500000  0.470588     0.500000\n",
      "4  0.600000       0.600000  0.533333  0.666667     0.600000\n",
      "\n",
      "\n",
      "random_forest:\n",
      "       base  undercomplete    sparse      deep  contractive\n",
      "0  1.000000       1.000000  0.928571  0.857143     1.000000\n",
      "1  0.647059       0.705882  0.735294  0.647059     0.735294\n",
      "2  0.777778       0.805556  0.833333  0.777778     0.805556\n",
      "3  0.705882       0.617647  0.705882  0.411765     0.588235\n",
      "4  0.566667       0.500000  0.566667  0.566667     0.733333\n",
      "\n",
      "\n",
      "naive_bayesian:\n",
      "       base  undercomplete    sparse      deep  contractive\n",
      "0  0.500000       0.571429  0.571429  0.535714     0.535714\n",
      "1  0.382353       0.500000  0.558824  0.500000     0.500000\n",
      "2  0.361111       0.500000  0.500000  0.555556     0.472222\n",
      "3  0.588235       0.529412  0.529412  0.529412     0.470588\n",
      "4  0.433333       0.566667  0.566667  0.533333     0.466667\n",
      "\n",
      "\n",
      "XGBoost:\n",
      "       base  undercomplete    sparse      deep  contractive\n",
      "0  1.000000       1.000000  1.000000  1.000000     1.000000\n",
      "1  0.647059       0.735294  0.794118  0.617647     0.617647\n",
      "2  0.666667       0.722222  0.777778  0.666667     0.583333\n",
      "3  0.735294       0.617647  0.588235  0.617647     0.529412\n",
      "4  0.600000       0.566667  0.566667  0.566667     0.566667\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in accuracies:\n",
    "    print(classifier + \":\")\n",
    "    print(pandas.DataFrame.from_dict(accuracies[classifier]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37talos",
   "language": "python",
   "name": "py37talos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
